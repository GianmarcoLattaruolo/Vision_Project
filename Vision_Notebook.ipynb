{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLeKGLqX_H4y"
      },
      "source": [
        "\n",
        "# Vision and Cognitive Systems - Project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GianmarcoLattaruolo/Vision_Project/blob/main/Vision_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q0q-cHAgX6g"
      },
      "source": [
        "# Preliminaries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BILxvIlegX6m"
      },
      "source": [
        "## Setting up the working space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmXSsJJKniqF"
      },
      "source": [
        "In this first cell we check if the notebook is runnig in Colab. In this case we need some additional work to set properly the environmet. We need also to mount our vision drive. In local machine instead we need to add the Geoestimation folder of our paper in the paths where python searches for libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSi1dIDk_H5R",
        "outputId": "c9db6b4d-d22e-4f15-d34b-5b094e803bde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "are we in Colab?: True\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m✨🍰✨ Everything looks OK!\n"
          ]
        }
      ],
      "source": [
        "# with this line we can check if we are in colab or not\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "print(\"are we in Colab?:\",in_colab)\n",
        "\n",
        "cwd = Path(os.getcwd())\n",
        "if in_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !pip install -q condacolab\n",
        "    import condacolab\n",
        "    condacolab.install()\n",
        "    os.chdir(cwd /'drive'/'MyDrive'/'GeoEstimation')\n",
        "else:\n",
        "    #our defult wd in local should be Vision_Project\n",
        "    if str(cwd)[-14:] == 'Vision_Project':\n",
        "        os.chdir(cwd / 'GeoEstimation')\n",
        "    sys.path.append(cwd / 'GeoEstimation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6EuxN1R_Zwi",
        "outputId": "57516044-99ec-4b09-e31a-44c13a89f11f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✨🍰✨ Everything looks OK!\n",
            "/content/drive/MyDrive/GeoEstimation\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ WARNING conda.core.solve:_add_specs(652): pinned spec cudatoolkit=11.6 conflicts with explicit specs.  Overriding pinned spec.\n",
            "\b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 22.9.0\n",
            "  latest version: 22.11.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "future-0.18.3        | 357 KB    | : 100% 1.0/1 [00:00<00:00,  6.13it/s]\n",
            "pandas-1.1.5         | 11.5 MB   | : 100% 1.0/1 [00:02<00:00,  2.25s/it]\n",
            "libtiff-4.5.0        | 397 KB    | : 100% 1.0/1 [00:00<00:00, 21.41it/s]\n",
            "libpng-1.6.39        | 276 KB    | : 100% 1.0/1 [00:00<00:00, 23.29it/s]\n",
            "packaging-23.0       | 40 KB     | : 100% 1.0/1 [00:00<00:00, 26.36it/s]\n",
            "blas-devel-3.9.0     | 12 KB     | : 100% 1.0/1 [00:00<00:00, 33.44it/s]\n",
            "werkzeug-2.2.3       | 247 KB    | : 100% 1.0/1 [00:00<00:00, 16.62it/s]\n",
            "libgfortran-ng-12.2. | 22 KB     | : 100% 1.0/1 [00:00<00:00, 26.92it/s]\n",
            "six-1.16.0           | 14 KB     | : 100% 1.0/1 [00:00<00:00, 33.89it/s]\n",
            "importlib-metadata-6 | 24 KB     | : 100% 1.0/1 [00:00<00:00, 26.81it/s]\n",
            "torchvision-0.7.0    | 10.9 MB   | : 100% 1.0/1 [00:01<00:00,  1.33s/it]\n",
            "jpeg-9e              | 235 KB    | : 100% 1.0/1 [00:00<00:00, 23.37it/s]\n",
            "attrs-22.2.0         | 53 KB     | : 100% 1.0/1 [00:00<00:00, 23.08it/s]\n",
            "pytorch-1.6.0        | 538.6 MB  | : 100% 1.0/1 [01:14<00:00, 74.10s/it] \n",
            "cudatoolkit-10.2.89  | 318.0 MB  | : 100% 1.0/1 [00:05<00:00,  5.83s/it]               \n",
            "pyjwt-2.6.0          | 21 KB     | : 100% 1.0/1 [00:00<00:00, 25.71it/s]\n",
            "lerc-4.0.0           | 275 KB    | : 100% 1.0/1 [00:00<00:00, 13.42it/s]\n",
            "libcblas-3.9.0       | 12 KB     | : 100% 1.0/1 [00:00<00:00, 30.06it/s]\n",
            "libdeflate-1.17      | 63 KB     | : 100% 1.0/1 [00:00<00:00, 25.98it/s]\n",
            "liblapacke-3.9.0     | 12 KB     | : 100% 1.0/1 [00:00<00:00, 28.60it/s]\n",
            "pyasn1-modules-0.2.7 | 60 KB     | : 100% 1.0/1 [00:00<00:00, 14.44it/s]\n",
            "libwebp-base-1.2.4   | 404 KB    | : 100% 1.0/1 [00:00<00:00, 11.34it/s]\n",
            "freetype-2.12.1      | 611 KB    | : 100% 1.0/1 [00:00<00:00, 17.11it/s]\n",
            "pytz-2022.7.1        | 182 KB    | : 100% 1.0/1 [00:00<00:00,  5.70it/s]\n",
            "blas-2.116           | 13 KB     | : 100% 1.0/1 [00:00<00:00, 31.25it/s]\n",
            "aiosignal-1.3.1      | 12 KB     | : 100% 1.0/1 [00:00<00:00, 25.77it/s]\n",
            "tensorboard-plugin-w | 668 KB    | : 100% 1.0/1 [00:00<00:00,  6.21it/s]\n",
            "python-dateutil-2.8. | 240 KB    | : 100% 1.0/1 [00:00<00:00, 18.70it/s]\n",
            "tensorboard-data-ser | 4.4 MB    | : 100% 1.0/1 [00:00<00:00,  7.53it/s]\n",
            "xorg-libxau-1.0.9    | 13 KB     | : 100% 1.0/1 [00:00<00:00, 33.49it/s]\n",
            "blinker-1.5          | 15 KB     | : 100% 1.0/1 [00:00<00:00, 25.28it/s]\n",
            "async-timeout-4.0.2  | 9 KB      | : 100% 1.0/1 [00:00<00:00, 32.57it/s]\n",
            "click-8.1.3          | 74 KB     | : 100% 1.0/1 [00:00<00:00, 23.52it/s]\n",
            "typing-extensions-4. | 8 KB      | : 100% 1.0/1 [00:00<00:00, 38.26it/s]\n",
            "mkl-devel-2022.1.0   | 25 KB     | : 100% 1.0/1 [00:00<00:00, 25.11it/s]\n",
            "pyyaml-6.0           | 183 KB    | : 100% 1.0/1 [00:00<00:00, 16.38it/s]\n",
            "tensorboard-2.12.0   | 5.0 MB    | : 100% 1.0/1 [00:00<00:00,  7.25it/s]\n",
            "fsspec-2023.1.0      | 106 KB    | : 100% 1.0/1 [00:00<00:00, 25.08it/s]\n",
            "zipp-3.13.0          | 17 KB     | : 100% 1.0/1 [00:00<00:00, 29.33it/s]\n",
            "typing_extensions-4. | 29 KB     | : 100% 1.0/1 [00:00<00:00, 30.71it/s]\n",
            "_openmp_mutex-4.5    | 6 KB      | : 100% 1.0/1 [00:00<00:00, 34.10it/s]\n",
            "python-3.8.16        | 21.8 MB   | : 100% 1.0/1 [00:01<00:00,  1.36s/it]               \n",
            "absl-py-1.4.0        | 100 KB    | : 100% 1.0/1 [00:00<00:00, 21.38it/s]\n",
            "libprotobuf-3.21.12  | 2.1 MB    | : 100% 1.0/1 [00:00<00:00,  6.55it/s]\n",
            "ca-certificates-2022 | 143 KB    | : 100% 1.0/1 [00:00<00:00, 28.99it/s]\n",
            "pyu2f-0.1.5          | 31 KB     | : 100% 1.0/1 [00:00<00:00, 25.00it/s]\n",
            "xorg-libxdmcp-1.1.3  | 19 KB     | : 100% 1.0/1 [00:00<00:00, 30.16it/s]\n",
            "libabseil-20230125.0 | 1.2 MB    | : 100% 1.0/1 [00:00<00:00,  2.42it/s]\n",
            "setuptools-59.8.0    | 1017 KB   | : 100% 1.0/1 [00:00<00:00,  2.87it/s]\n",
            "grpcio-1.52.1        | 757 KB    | : 100% 1.0/1 [00:00<00:00, 10.41it/s]\n",
            "zlib-1.2.13          | 92 KB     | : 100% 1.0/1 [00:00<00:00, 17.24it/s]\n",
            "yarl-1.8.2           | 88 KB     | : 100% 1.0/1 [00:00<00:00, 17.21it/s]\n",
            "libgfortran5-12.2.0  | 1.8 MB    | : 100% 1.0/1 [00:00<00:00,  2.58it/s]\n",
            "tbb-2021.7.0         | 1.5 MB    | : 100% 1.0/1 [00:00<00:00, 12.94it/s]\n",
            "tqdm-4.50.2          | 55 KB     | : 100% 1.0/1 [00:00<00:00, 13.94it/s]\n",
            "protobuf-4.21.12     | 315 KB    | : 100% 1.0/1 [00:00<00:00, 10.08it/s]\n",
            "pthread-stubs-0.4    | 5 KB      | : 100% 1.0/1 [00:00<00:00, 32.03it/s]\n",
            "libxcb-1.13          | 391 KB    | : 100% 1.0/1 [00:00<00:00,  5.28it/s]\n",
            "markdown-3.4.1       | 65 KB     | : 100% 1.0/1 [00:00<00:00, 14.51it/s]\n",
            "rsa-4.9              | 29 KB     | : 100% 1.0/1 [00:00<00:00, 19.89it/s]\n",
            "markupsafe-2.1.2     | 23 KB     | : 100% 1.0/1 [00:00<00:00, 20.01it/s]\n",
            "libhwloc-2.8.0       | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.70it/s]\n",
            "requests-oauthlib-1. | 22 KB     | : 100% 1.0/1 [00:00<00:00, 24.11it/s]\n",
            "aiohttp-3.8.4        | 435 KB    | : 100% 1.0/1 [00:00<00:00, 10.91it/s]\n",
            "openjpeg-2.5.0       | 344 KB    | : 100% 1.0/1 [00:00<00:00, 21.02it/s]\n",
            "liblapack-3.9.0      | 12 KB     | : 100% 1.0/1 [00:00<00:00, 21.48it/s]\n",
            "frozenlist-1.3.3     | 45 KB     | : 100% 1.0/1 [00:00<00:00, 26.09it/s]\n",
            "certifi-2022.12.7    | 147 KB    | : 100% 1.0/1 [00:00<00:00, 25.71it/s]\n",
            "multidict-6.0.4      | 52 KB     | : 100% 1.0/1 [00:00<00:00, 19.02it/s]\n",
            "pip-23.0             | 1.3 MB    | : 100% 1.0/1 [00:00<00:00,  3.45it/s]\n",
            "pytorch-lightning-1. | 186 KB    | : 100% 1.0/1 [00:00<00:00,  5.93it/s]\n",
            "libgrpc-1.52.1       | 5.3 MB    | : 100% 1.0/1 [00:00<00:00,  1.56it/s]\n",
            "google-auth-2.16.0   | 97 KB     | : 100% 1.0/1 [00:00<00:00, 13.68it/s]\n",
            "lcms2-2.14           | 235 KB    | : 100% 1.0/1 [00:00<00:00, 22.05it/s]\n",
            "google-auth-oauthlib | 19 KB     | : 100% 1.0/1 [00:00<00:00, 21.81it/s]\n",
            "openssl-3.0.8        | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  5.64it/s]\n",
            "cachetools-5.3.0     | 14 KB     | : 100% 1.0/1 [00:00<00:00, 23.78it/s]\n",
            "oauthlib-3.2.2       | 90 KB     | : 100% 1.0/1 [00:00<00:00, 10.24it/s]\n",
            "ninja-1.11.0         | 2.8 MB    | : 100% 1.0/1 [00:00<00:00,  1.72it/s]\n",
            "mkl-2022.1.0         | 199.6 MB  | : 100% 1.0/1 [00:36<00:00, 36.10s/it]               \n",
            "libblas-3.9.0        | 13 KB     | : 100% 1.0/1 [00:00<00:00, 21.76it/s]\n",
            "mkl-include-2022.1.0 | 745 KB    | : 100% 1.0/1 [00:00<00:00,  1.97it/s]\n",
            "numpy-1.24.2         | 6.3 MB    | : 100% 1.0/1 [00:00<00:00,  1.18it/s]\n",
            "msgpack-python-1.0.0 | 93 KB     | : 100% 1.0/1 [00:00<00:00, 19.08it/s]\n",
            "llvm-openmp-15.0.7   | 3.1 MB    | : 100% 1.0/1 [00:00<00:00,  8.00it/s]\n",
            "re2-2023.02.01       | 196 KB    | : 100% 1.0/1 [00:00<00:00, 16.85it/s]\n",
            "pyasn1-0.4.8         | 53 KB     | : 100% 1.0/1 [00:00<00:00, 17.07it/s]\n",
            "pillow-9.4.0         | 43.8 MB   | : 100% 1.0/1 [00:01<00:00,  1.25s/it]               \n",
            "Preparing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
            "\n",
            "\b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Installing pip dependencies: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- Ran pip subprocess with arguments:\n",
            "['/usr/local/bin/python', '-m', 'pip', 'install', '-U', '-r', '/content/drive/MyDrive/GeoEstimation/condaenv.4rh0k2ba.requirements.txt']\n",
            "Pip subprocess output:\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting s2sphere==0.2.5\n",
            "  Downloading s2sphere-0.2.5-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: future>=0.15 in /usr/local/lib/python3.8/site-packages (from s2sphere==0.2.5->-r /content/drive/MyDrive/GeoEstimation/condaenv.4rh0k2ba.requirements.txt (line 1)) (0.18.3)\n",
            "Installing collected packages: s2sphere\n",
            "Successfully installed s2sphere-0.2.5\n",
            "\n",
            "\b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate base\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "Retrieving notices: ...working... done\n",
            "\u001b[33mWARNING: Skipping torchtext as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.7\n",
            "  Downloading torchtext-0.7.0-cp38-cp38-manylinux1_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from torchtext==0.7) (2.28.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/site-packages (from torchtext==0.7) (1.6.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from torchtext==0.7) (4.50.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (from torchtext==0.7) (1.24.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->torchtext==0.7) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/site-packages (from requests->torchtext==0.7) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->torchtext==0.7) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->torchtext==0.7) (1.26.13)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/site-packages (from torch->torchtext==0.7) (0.18.3)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "Successfully installed sentencepiece-0.1.97 torchtext-0.7.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# this cell takes a lot of time on colab!\n",
        "import sys\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "if in_colab:\n",
        "    import condacolab\n",
        "    condacolab.check()\n",
        "    import os\n",
        "    os.chdir(r'/content/drive/MyDrive/GeoEstimation')\n",
        "    print(os.getcwd())\n",
        "    !conda env update -n base -f environment.yml\n",
        "    # The following is ridiculous, I know, but it seems to work\n",
        "    !pip uninstall torchtext\n",
        "    !pip install torchtext==0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSyJGMw23UYv"
      },
      "source": [
        "In theory we need to install some specific packages with certain version to account for the original environment in which the paper results were obtained:\n",
        "```\n",
        "  - python=3.8\n",
        "  - msgpack-python=1.0.0\n",
        "  - pandas=1.1.5\n",
        "  - yaml=0.2.5\n",
        "  - tqdm=4.50\n",
        "  - cudatoolkit=10.2\n",
        "  - pytorch=1.6\n",
        "  - torchvision=0.7\n",
        "  - pytorch-lightning=1.0.1\n",
        "  - pip\n",
        "  - pip:\n",
        "    - s2sphere==0.2.5\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0oiwBBngX7A"
      },
      "source": [
        "## A transfer learning example: the strenght of pytorch-lightning\n",
        "\n",
        "Here we want to show in a nutshell the transfer learning approach from a pretrained model using both standard code and pytorch-lightning, to highlight the differences. Moreover we are going to load the same pretrained model (ResNet50) used by the authors as backbone to develop their ML model. For seek of semplicity we are going to re-train this model on the Cifar10. First let's see the classic torch approach:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6b33d04d11d4470cbfcf6326531e6fa1",
            "3e7931760b03424d897b5c635af54832",
            "31bed04aa41b43a9ac14928426d3fc73",
            "6e46a2b63ede4ebb937d9a48a14ef355",
            "ec91d6421db149568f4707a854040869",
            "8973c26d36c3435c95d19ece007c8f81",
            "68a2181ae5794a3a8b48c60895384986",
            "f2f0549a63f64823a434168c6b96eb3f",
            "56e9dc76e0384e52ba4cb2eadb17f151",
            "6c40c00c0f6648609f56e7c10022fea6",
            "5ccff7a9383b403f9ddc1b6c29f490f8",
            "6298babe16614ae0bf724356680f9ae0",
            "190ed9a6ee42446db2a2d45506f61e7e",
            "a59d401859a04cef9d41162904d728d7",
            "1ac2579ce8f94562a74ae7f048885edc",
            "8c5e8b9cc38e479e8d1d2c231cbf0ffa",
            "f30295f9dba04eca89dddc062a3ed881",
            "7edb1455eb8347288073c720eb8ebb95",
            "125227802eef4216b7d9264a3e994196",
            "5e3bc8c827784295bef2fbe50424a816",
            "b595806ae0424fc6b4a3369ab5836cc7",
            "67c610b42cff429382fbfa495010f482"
          ]
        },
        "id": "ia68yVpogX7F",
        "outputId": "6cb758a1-4f96-4208-e3f9-dd15bb42ff7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b33d04d11d4470cbfcf6326531e6fa1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6298babe16614ae0bf724356680f9ae0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "Epoch 0\n",
            "3.1930480003356934\n",
            "3.123041868209839\n",
            "3.0640506744384766\n",
            "3.0658154487609863\n",
            "3.264195442199707\n",
            "3.7586264610290527\n",
            "3.468670606613159\n",
            "3.4459760189056396\n",
            "3.164868116378784\n",
            "3.2048826217651367\n",
            "Epoch 1\n",
            "3.4092202186584473\n",
            "3.2618446350097656\n",
            "3.543487787246704\n",
            "2.9497785568237305\n",
            "3.0474600791931152\n",
            "3.175663709640503\n",
            "2.6673035621643066\n",
            "3.1997504234313965\n",
            "2.9967129230499268\n",
            "3.018550395965576\n",
            "Epoch 2\n",
            "3.0200114250183105\n",
            "3.6485610008239746\n",
            "2.8175318241119385\n",
            "3.3879618644714355\n",
            "2.957050323486328\n",
            "2.942509889602661\n",
            "3.0313799381256104\n",
            "2.8381459712982178\n",
            "2.9101810455322266\n",
            "2.69409441947937\n",
            "Epoch 3\n",
            "3.0937843322753906\n",
            "2.800339698791504\n",
            "2.7473137378692627\n",
            "2.8794608116149902\n",
            "2.829761266708374\n",
            "2.7380940914154053\n",
            "2.991612434387207\n",
            "2.689542770385742\n",
            "2.536581516265869\n",
            "3.066215991973877\n",
            "Epoch 4\n",
            "2.942972421646118\n",
            "2.8620858192443848\n",
            "2.899718999862671\n",
            "2.8575029373168945\n",
            "2.734931707382202\n",
            "2.764251708984375\n",
            "2.7208406925201416\n",
            "2.7029600143432617\n",
            "2.576611042022705\n",
            "2.772939920425415\n",
            "Epoch 5\n",
            "2.8225460052490234\n",
            "2.631972074508667\n",
            "2.8054146766662598\n",
            "2.5750010013580322\n",
            "2.774996280670166\n",
            "2.6839115619659424\n",
            "2.857142686843872\n",
            "2.3621490001678467\n",
            "2.97297739982605\n",
            "2.690294027328491\n",
            "Epoch 6\n",
            "2.7450766563415527\n",
            "3.1124308109283447\n",
            "2.525341510772705\n",
            "2.5314621925354004\n",
            "2.88797664642334\n",
            "2.4406018257141113\n",
            "2.2911229133605957\n",
            "2.389775514602661\n",
            "2.615004062652588\n",
            "2.8636035919189453\n",
            "Epoch 7\n",
            "2.6630327701568604\n",
            "2.590579032897949\n",
            "2.5763955116271973\n",
            "2.500095844268799\n",
            "2.5076839923858643\n",
            "2.598517656326294\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d719591ce19d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m               \u001b[0;31m#(b, 3, 32, 32) -> (b, 1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m               \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m           \u001b[0;31m# (b, 1000) -> (b, 10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#libraries\n",
        "from torchvision import models\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import softmax, cross_entropy\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "#check for GPU\n",
        "want_gpu = True\n",
        "if want_gpu and torch.cuda.is_available():\n",
        "    gpu = 1\n",
        "else:\n",
        "    gpu = None\n",
        "\n",
        "#download the pretrained model\n",
        "backbone = models.resnet50(pretrained = True)\n",
        "\n",
        "#download and normalize the CIFAR10 dataset\n",
        "normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                 std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
        "cf10_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "cifar_10 = CIFAR10('.',train=True, download = True, transform=cf10_transforms) \n",
        "\n",
        "'''#train, validation and test split (we have to set the seed)\n",
        "train_data, val_data, test_data = random_split(cifar_10, [40000, 10000, 10000] )'''\n",
        "\n",
        "#prepare the batches\n",
        "train_loader = DataLoader(cifar_10, batch_size=32, shuffle=True)\n",
        "'''val_loader = DataLoader(val_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=True)'''\n",
        "\n",
        "# We add to the last layer with a fully connected one to match our number of classes (=10):\n",
        "# We treat the outputs of resnet as high level features (we could use them with any classifier instead of a FC)\n",
        "finetune_layer = torch.nn.Linear(backbone.fc.out_features, 10) \n",
        "#finetune_layer = torch.nn.Linear(backbone.fc.in_features, 10) is for REPLACE THE LAST LAYER\n",
        "\n",
        "#define the optimizer\n",
        "optimizer = Adam(finetune_layer.parameters(), lr = 1e-4)\n",
        "\n",
        "#we set a limit for the number of batches in each epoch, since we are not interesting in training proper this model\n",
        "limit_train_batches = 10\n",
        "\n",
        "#training\n",
        "for epoch in range(10):\n",
        "    print(f'Epoch {epoch}')\n",
        "    for i,batch in enumerate(train_loader):\n",
        "      if i<limit_train_batches:\n",
        "\n",
        "          x, y = batch\n",
        "          #we do not waste memory recording the gradient on the backbone\n",
        "          with torch.no_grad():\n",
        "              #(b, 3, 32, 32) -> (b, 1000)\n",
        "              features = backbone(x)\n",
        "\n",
        "          # (b, 1000) -> (b, 10)\n",
        "          preds = finetune_layer(features)\n",
        "          loss = cross_entropy(preds, y)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "          print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Rf6CfUHgX7L"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.metrics.functional import accuracy\n",
        "\n",
        "class ImageClassifier(pl.LightningModule):\n",
        "    def __init__(self, num_classes=10 , lr = 1e-3):\n",
        "        super().__init__()\n",
        "        #this setting save as the time to define an attribute for each hyperparameter --> self.hparams.<parameter>\n",
        "        self.save_hyperparameters() #Pytorch-lightning trick!\n",
        "        self.backbone = models.resnet50(pretrained = True)\n",
        "        self.finetune_layer = torch.nn.Linear(backbone.fc.out_features, num_classes)\n",
        "\n",
        "    def training_step(self, batch, batch_idx): #these methods are standard methods in LightningModule\n",
        "        x, y = batch\n",
        "\n",
        "        #we decide whether to freeze the backbone or not on the base of the number of epochs\n",
        "        if self.trainer.current_epoch < 10:\n",
        "            with torch.no_grad():\n",
        "                #(b, 3, 32, 32) -> (b, 1000)\n",
        "                features = self.backbone(x)\n",
        "        else:\n",
        "            features = self.backbone(x)\n",
        "\n",
        "        # (b, 1000) -> (b, 10)\n",
        "        preds = self.finetune_layer(features)\n",
        "        loss = cross_entropy(preds, y)\n",
        "        #we don't need anymore loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
        "        self.log('train_loss', loss) # we will see later this method of LightningModule\n",
        "        self.log('train_loss', accuracy(preds, y))\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # (b, 1000) -> (b, 10)\n",
        "        preds = self.finetune_layer(features)\n",
        "        loss = cross_entropy(preds, y)\n",
        "        #we don't need anymore loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
        "        self.log('val_loss', loss) # we will see later this method of LightningModule\n",
        "        self.log('val_loss', accuracy(preds, y))\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = Adam(self.parameters(), lr =self.hparams.lr) \n",
        "        #we can safely pass all the parameters since in the backbone we are not computing the gradient\n",
        "        return optimizer\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv9Yxw-AgX7R"
      },
      "source": [
        "At this point we have  very flexible object ```ImageClassifier```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1ig2I1hgX7U"
      },
      "outputs": [],
      "source": [
        "from pl_bolts.datamodules import CIFAR10DataModule\n",
        "\n",
        "#Bolts save us the time of train, vla, test split and using 3 different torch.Dataloader for each of them\n",
        "dm = CIFAR10DataModule('.') \n",
        "\n",
        "classifier = ImageClassifier()\n",
        "logger = pl.loggers.TensorBoardLogger(name = f'pretrained model 1', save_dir = 'lightning_logs')\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs = 2, # set the number of epochs if <1000 (=defult)\n",
        "    progress_bar_refresh_rate = 20, \n",
        "    logger = logger,\n",
        "    #gpus=1, \n",
        "    limit_train_batches = 50,\n",
        "    #limit_val_batches = 2,\n",
        "    #check_val_every_n_epoch = 5\n",
        "    #fast_dev_run=True # add this to have a fast chech of bugs\n",
        "    ) \n",
        "trainer.fit(classifier, dm) #we can use the normal train_loader we defined previously\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkDLLwMegX7X"
      },
      "source": [
        "We can use this very nice [tool](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.loggers.tensorboard.html#module-pytorch_lightning.loggers.tensorboard) form Pytorch-Lighting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1EhLmc7gX7Z"
      },
      "outputs": [],
      "source": [
        "# start tensorboard\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgm93PqcgX7b"
      },
      "source": [
        "### Self-supervised transfer learning with Lightning\n",
        "\n",
        "PyTorch Lightning implementation of SwAV adapted from the [official implementation](https://arxiv.org/abs/2006.09882), whose authors used the same pretrained model (ResNet50 trained on ImageNet). We can simply import this model from Lightning-Bolt and define a class very similar to the previous one for our classifer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FC_NgnkgX7c"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import softmax, cross_entropy\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import random_split\n",
        "import pytorch_lightning as pl\n",
        "from pl_bolts.models.self_supervised import SwAV\n",
        "from pl_bolts.datamodules import CIFAR10DataModule\n",
        "\n",
        "#Bolts save us the time of train, vla, test split and using 3 different torch.Dataloader for each of them\n",
        "dm = CIFAR10DataModule('.') \n",
        "\n",
        "#weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/swav/bolts_swav_imagenet/swav_imagenet.ckpt'\n",
        "#weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/swav/swav_imagenet/swav_imagenet.pth.tar'\n",
        "swav = SwAV.load_from_checkpoint(r'C:\\Users\\latta\\.cache\\torch\\hub\\checkpoints\\swav_imagenet.pth.tar', strict=False)\n",
        "\n",
        "class SSLImageClassifier(pl.LightningModule):\n",
        "    def __init__(self, num_classes=10 , lr = 1e-3):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters() \n",
        "        self.backbone = swav.model #model pretrained on ImageNet without labels\n",
        "        self.finetune_layer = torch.nn.Linear(3000, num_classes)\n",
        "\n",
        "    def training_step(self, batch, batch_idx): #these methods are standard methods in \n",
        "        x, y = batch\n",
        "\n",
        "        #we decide whether to freeze the backbone or not on the base of the number of epochs\n",
        "        if self.trainer.current_epoch < 10:\n",
        "            with torch.no_grad():\n",
        "                #(b, 3, 32, 32) -> (b, 1000)\n",
        "                (f1, f2) = self.backbone(x)\n",
        "                features = f2\n",
        "        else:\n",
        "            (f1, f2) = self.backbone(x)\n",
        "            features = f2\n",
        "\n",
        "        # (b, 1000) -> (b, 10)\n",
        "        preds = self.finetune_layer(features)\n",
        "        loss = cross_entropy(preds, y)\n",
        "        #we don't need anymore loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
        "        self.log('train_loss', loss) # we will see later this method of LightningModule\n",
        "        self.log('train_loss', accuracy(preds, y))\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx): #these methods are standard methods in \n",
        "        x, y = batch\n",
        "        \n",
        "        (f1, f2) = self.backbone(x)\n",
        "        features = f2\n",
        "\n",
        "        # (b, 1000) -> (b, 10)\n",
        "        preds = self.finetune_layer(features)\n",
        "        loss = cross_entropy(preds, y)\n",
        "        #we don't need anymore loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
        "        self.log('val_loss', loss) # we will see later this method of LightningModule\n",
        "        self.log('val_loss', accuracy(preds, y))\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = Adam(self.parameters(), lr =self.hparams.lr) \n",
        "        #we can safely pass all the parameters since in the backbone we are not computing the gradient\n",
        "        return optimizer\n",
        "\n",
        "'''backbone = models.resnet50(pretrained = True)\n",
        "finetune_layer = torch.nn.Linear(backbone.fc.out_features, 10) \n",
        "#define the optimizer\n",
        "optimizer = Adam(finetune_layer.parameters(), lr = 1e-4)\n",
        "'''\n",
        "ssl_classifier = SSLImageClassifier()\n",
        "logger = pl.loggers.TensorBoardLogger(name = f'pretrained model 2 (self superised)', save_dir = 'lightning_logs')\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs = 2,\n",
        "    #progress_bar_refresh_rate = 20, \n",
        "    gpus=0, \n",
        "    limit_train_batches = 50#, \n",
        "    #fast_dev_run=True # add this to have a fast chech of bugs\n",
        "    ) \n",
        "trainer.fit(ssl_classifier, dm) #we can use the normal train_loader we defined previously"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d5LUBSEgX7i"
      },
      "outputs": [],
      "source": [
        "# start tensorboard\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4BF4QseoP35"
      },
      "source": [
        "## Reproduce paper results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwz47JYx_H5b"
      },
      "source": [
        "To begin we try to reproduce the paper results on their test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay_jd1lT_H5d"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from math import ceil\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from classification.train_base import MultiPartitioningClassifier # class defining our model\n",
        "from classification.dataset import FiveCropImageDataset # class for preparing the images before giving them to the NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiYqpWSn_H5i"
      },
      "source": [
        "### Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s09ZTd3_H5k"
      },
      "outputs": [],
      "source": [
        "# where model's params and hyperparams are saved\n",
        "checkpoint = \"models/base_M/epoch=014-val_loss=18.4833.ckpt\"\n",
        "hparams = \"models/base_M/hparams.yaml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IsHtRs6_H5m"
      },
      "outputs": [],
      "source": [
        "# load_from_checkpoint is a static method from pytorch lightning, inherited by MultiPartitioningClassifier\n",
        "# it permits to load a model previously saved, in the form of a checkpoint file, and one with hyperparameters\n",
        "# MultiPartitioningClassifier is the class defining our model\n",
        "model = MultiPartitioningClassifier.load_from_checkpoint(\n",
        "    checkpoint_path=checkpoint,\n",
        "    hparams_file=hparams,\n",
        "    map_location=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqaPzJCygX7s"
      },
      "outputs": [],
      "source": [
        "type(pl.LightningModule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lANl7PDx_H5o"
      },
      "outputs": [],
      "source": [
        "#to allow GPU\n",
        "want_gpu = True\n",
        "if want_gpu and torch.cuda.is_available():\n",
        "    gpu = 1\n",
        "else:\n",
        "    gpu = None\n",
        "\n",
        "# the class Trainer from pythorch lightining is the one responsible for training a deep NN\n",
        "# it can initialize the model, run forward and backward passes, optimize, print stats, early stop...\n",
        "wanted_precision = 32 #16 for half precision (how many bits for each number)\n",
        "trainer = pl.Trainer(gpus=gpu, precision=wanted_precision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csYOzf4r_H5p"
      },
      "source": [
        "### Load and initialize the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdLRVDHC_H5q"
      },
      "outputs": [],
      "source": [
        "# where images are saved\n",
        "image_dir = \"resources/images/im2gps\"\n",
        "meta_csv = \"resources/images/im2gps_places365.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5WRFwx0gX74"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "first_csv = pd.read_csv(meta_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf0Rm1h8_H5r"
      },
      "outputs": [],
      "source": [
        "#FiveCropImageDataset is the class for preparing the images before giving them to the NN\n",
        "# in particular, it creates five different crops for every image\n",
        "dataset = FiveCropImageDataset(meta_csv, image_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNIX_tu2_H5s"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "                    dataset,\n",
        "                    batch_size=ceil(batch_size / 5),  #you divide by 5 because for each image you generate 5 different crops\n",
        "                    shuffle=False,\n",
        "                    num_workers=4 #number ot threads used for parallelism (cores of CPU?)\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQycvZEH_H5s"
      },
      "source": [
        "### Run the model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwfspFbj_H5t"
      },
      "outputs": [],
      "source": [
        "results = trainer.test(model, test_dataloaders=dataloader, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu5nzy7B_H5u"
      },
      "source": [
        "### Look at the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OHtm_1M_H5u"
      },
      "outputs": [],
      "source": [
        "# formatting results into a pandas dataframe\n",
        "df = pd.DataFrame(results[0]).T\n",
        "#df[\"dataset\"] = image_dir\n",
        "df[\"partitioning\"] = df.index\n",
        "df[\"partitioning\"] = df[\"partitioning\"].apply(lambda x: x.split(\"/\")[-1])\n",
        "df.set_index(keys=[\"partitioning\"], inplace=True) #keys=[\"dataset\", \"partitioning\"] in case\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6vSJgR0_H5v"
      },
      "outputs": [],
      "source": [
        "# to save the dataframe on a csv file\n",
        "fout = 'test_results.csv'\n",
        "df.to_csv(fout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2g9datY1blm"
      },
      "outputs": [],
      "source": [
        "os.chdir(r'/content/drive/MyDrive/GeoEstimation/resources/images/im2gps')\n",
        "print(len(os.listdir()))\n",
        "os.chdir(r'/content/drive/MyDrive/GeoEstimation')\n",
        "print(os.getcwd())\n",
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "# Output would be True if Pytorch is using GPU otherwise it would be False.\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B3kpB3fnTzR",
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#libraries to import\n",
        "#known\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import torchvision\n",
        "import torch\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "import sys\n",
        "import time\n",
        "from math import ceil\n",
        "\n",
        "\n",
        "\n",
        "#Unknown\n",
        "from typing import Union\n",
        "from io import BytesIO\n",
        "import random\n",
        "from argparse import Namespace, ArgumentParser\n",
        "from pathlib import Path\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "import requests\n",
        "import logging\n",
        "import json\n",
        "import yaml\n",
        "from tqdm.auto import tqdm\n",
        "#from classification.train_base import MultiPartitioningClassifier\n",
        "#from classification.dataset import FiveCropImageDataset\n",
        "\n",
        "#to divide\n",
        "from classification import utils_global\n",
        "from classification.s2_utils import Partitioning, Hierarchy\n",
        "from classification.dataset import MsgPackIterableDatasetMultiTargetWithDynLabels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFcvIDD5nTzr"
      },
      "source": [
        "The main link and paper that we need to follow is [this](https://github.com/TIBHannover/GeoEstimation) and [this](https://github.com/TIBHannover/GeoEstimation/releases/) for the pretrained models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e56Y49vfnTzr"
      },
      "source": [
        "Davide ha trovato questo che forse è meglio [kaggle](https://www.kaggle.com/code/habedi/inspect-the-dataset/data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0OQXXljgX8T"
      },
      "source": [
        "## Our dataset\n",
        "\n",
        "We have downloaded a new 10k dataset with gps coordinates. We need the labels for the scenes.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import importlib\n",
        "imported_module = importlib.import_module(\"scene_classification\")\n",
        "importlib.reload(imported_module)\n",
        "import scene_classification\n",
        "from scene_classification import SceneClassifier\n"
      ],
      "metadata": {
        "id": "Q-pBnlfClJDN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wnCXTaVggX8V",
        "outputId": "ca6c9502-e8db-474d-d359-0d934af2ab83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading scene hierarchy ...\n",
            "num of images is  9446\n",
            "the origninal df of images info is\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Unnamed: 0     photo_id          owner  gender occupation  \\\n",
              "0           0  17271526139  130418712@N05     1.0        NaN   \n",
              "1           1  17776887679   55101137@N02     1.0        NaN   \n",
              "2           2  17898331633   55101137@N02     1.0        NaN   \n",
              "3           3  17940239919   55101137@N02     1.0        NaN   \n",
              "4           4  17963122505   55101137@N02     1.0        NaN   \n",
              "\n",
              "                     title                                        description  \\\n",
              "0                Rio Trejo  Son numerosos los rios y arroyos que discurren...   \n",
              "1  2015-05-13-022FD PH-XRD  <u><b>Aircraft Type - Registration - (c/n)</b>...   \n",
              "2  2015-05-17-022FD OO-GWA  <u><b>Aircraft Type - Registration - (c/n)</b>...   \n",
              "3  2015-05-14-020FD D-1553  <u><b>Aircraft Type - Registration - (c/n)</b>...   \n",
              "4  2015-05-13-025FD EI-DLI  <u><b>Aircraft Type - Registration - (c/n)</b>...   \n",
              "\n",
              "   faves        lat        lon  u_city       u_country                taken  \\\n",
              "0  701.0  36,861544  -5,177747     NaN             NaN  2015-04-26 17:11:11   \n",
              "1    1.0  51,463766   5,392935  Bodmin  United Kingdom  2015-05-13 00:00:22   \n",
              "2    2.0  51,190492   4,453765  Bodmin  United Kingdom  2015-05-17 00:00:22   \n",
              "3    0.0  51,326247   6,085953  Bodmin  United Kingdom  2015-05-14 00:00:20   \n",
              "4    2.0  51,463766   5,392935  Bodmin  United Kingdom  2015-05-13 00:00:25   \n",
              "\n",
              "   weather  season  daytime  \\\n",
              "0      NaN     1.0      2.0   \n",
              "1      9.0     1.0      3.0   \n",
              "2      9.0     1.0      3.0   \n",
              "3      9.0     1.0      3.0   \n",
              "4      9.0     1.0      3.0   \n",
              "\n",
              "                                            base_url  \\\n",
              "0  https://www.flickr.com/photos/130418712@N05/17...   \n",
              "1  https://www.flickr.com/photos/55101137@N02/177...   \n",
              "2  https://www.flickr.com/photos/55101137@N02/178...   \n",
              "3  https://www.flickr.com/photos/55101137@N02/179...   \n",
              "4  https://www.flickr.com/photos/55101137@N02/179...   \n",
              "\n",
              "                                                 url  \n",
              "0  https://live.staticflickr.com/65535/1727152613...  \n",
              "1  https://live.staticflickr.com/5335/17776887679...  \n",
              "2  https://live.staticflickr.com/525/17898331633_...  \n",
              "3  https://live.staticflickr.com/8860/17940239919...  \n",
              "4  https://live.staticflickr.com/5457/17963122505...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-905f0d3f-11cb-40e0-ad9e-7512d92d6615\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>photo_id</th>\n",
              "      <th>owner</th>\n",
              "      <th>gender</th>\n",
              "      <th>occupation</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>faves</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>u_city</th>\n",
              "      <th>u_country</th>\n",
              "      <th>taken</th>\n",
              "      <th>weather</th>\n",
              "      <th>season</th>\n",
              "      <th>daytime</th>\n",
              "      <th>base_url</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>17271526139</td>\n",
              "      <td>130418712@N05</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Rio Trejo</td>\n",
              "      <td>Son numerosos los rios y arroyos que discurren...</td>\n",
              "      <td>701.0</td>\n",
              "      <td>36,861544</td>\n",
              "      <td>-5,177747</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-04-26 17:11:11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>https://www.flickr.com/photos/130418712@N05/17...</td>\n",
              "      <td>https://live.staticflickr.com/65535/1727152613...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>17776887679</td>\n",
              "      <td>55101137@N02</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-05-13-022FD PH-XRD</td>\n",
              "      <td>&lt;u&gt;&lt;b&gt;Aircraft Type - Registration - (c/n)&lt;/b&gt;...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51,463766</td>\n",
              "      <td>5,392935</td>\n",
              "      <td>Bodmin</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>2015-05-13 00:00:22</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>https://www.flickr.com/photos/55101137@N02/177...</td>\n",
              "      <td>https://live.staticflickr.com/5335/17776887679...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>17898331633</td>\n",
              "      <td>55101137@N02</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-05-17-022FD OO-GWA</td>\n",
              "      <td>&lt;u&gt;&lt;b&gt;Aircraft Type - Registration - (c/n)&lt;/b&gt;...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>51,190492</td>\n",
              "      <td>4,453765</td>\n",
              "      <td>Bodmin</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>2015-05-17 00:00:22</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>https://www.flickr.com/photos/55101137@N02/178...</td>\n",
              "      <td>https://live.staticflickr.com/525/17898331633_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>17940239919</td>\n",
              "      <td>55101137@N02</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-05-14-020FD D-1553</td>\n",
              "      <td>&lt;u&gt;&lt;b&gt;Aircraft Type - Registration - (c/n)&lt;/b&gt;...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>51,326247</td>\n",
              "      <td>6,085953</td>\n",
              "      <td>Bodmin</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>2015-05-14 00:00:20</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>https://www.flickr.com/photos/55101137@N02/179...</td>\n",
              "      <td>https://live.staticflickr.com/8860/17940239919...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>17963122505</td>\n",
              "      <td>55101137@N02</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-05-13-025FD EI-DLI</td>\n",
              "      <td>&lt;u&gt;&lt;b&gt;Aircraft Type - Registration - (c/n)&lt;/b&gt;...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>51,463766</td>\n",
              "      <td>5,392935</td>\n",
              "      <td>Bodmin</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>2015-05-13 00:00:25</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>https://www.flickr.com/photos/55101137@N02/179...</td>\n",
              "      <td>https://live.staticflickr.com/5457/17963122505...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-905f0d3f-11cb-40e0-ad9e-7512d92d6615')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-905f0d3f-11cb-40e0-ad9e-7512d92d6615 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-905f0d3f-11cb-40e0-ad9e-7512d92d6615');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 74 batches\n",
            "We are at batch  1\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  2\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  3\n",
            "this batch has size  torch.Size([127, 3, 256, 256])\n",
            "We are at batch  4\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  5\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  6\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  7\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  8\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  9\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  10\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  11\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  12\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  13\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  14\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  15\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  16\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  17\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  18\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  19\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  20\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  21\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  22\n",
            "this batch has size  torch.Size([127, 3, 256, 256])\n",
            "We are at batch  23\n",
            "this batch has size  torch.Size([127, 3, 256, 256])\n",
            "We are at batch  24\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  25\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  26\n",
            "this batch has size  torch.Size([127, 3, 256, 256])\n",
            "We are at batch  27\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  28\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  29\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  30\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  31\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  32\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  33\n",
            "this batch has size  torch.Size([127, 3, 256, 256])\n",
            "We are at batch  34\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  35\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  36\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  37\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  38\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  39\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  40\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  41\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  42\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  43\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  44\n",
            "this batch has size  torch.Size([127, 3, 256, 256])\n",
            "We are at batch  45\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  46\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  47\n",
            "this batch has size  torch.Size([127, 3, 256, 256])\n",
            "We are at batch  48\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  49\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  50\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  51\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  52\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  53\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  54\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  55\n",
            "this batch has size  torch.Size([127, 3, 256, 256])\n",
            "We are at batch  56\n",
            "this batch has size  torch.Size([126, 3, 256, 256])\n",
            "We are at batch  57\n",
            "this batch has size  torch.Size([127, 3, 256, 256])\n",
            "We are at batch  58\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  59\n",
            "this batch has size  torch.Size([126, 3, 256, 256])\n",
            "We are at batch  60\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  61\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  62\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  63\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  64\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  65\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  66\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  67\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  68\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  69\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  70\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  71\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  72\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  73\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  74\n",
            "this batch has size  torch.Size([102, 3, 256, 256])\n",
            "num of triplette of probabilities is  9433\n",
            "num of  S3_labels is  9433\n",
            "the new data column we have now are\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       photo_id  S3_label  Prob_indoor  Prob_natural  Prob_urban\n",
              "0   47386036511       2.0  -146.067384    -80.056533  226.165605\n",
              "1   47386038251       2.0  -184.546766    -62.174156  246.758862\n",
              "2   47386039291       2.0  -113.993057    -65.118958  179.150011\n",
              "3   47386040661       2.0  -152.126420    -56.406855  208.573240\n",
              "4   47386048321       0.0   240.147499   -147.099608  -93.008178\n",
              "5   47386064931       2.0  -368.825431    124.437545  244.419923\n",
              "6   47386094261       1.0  -238.672925    185.648652   53.072303\n",
              "7   47386176281       2.0     1.440025   -100.908386   99.508767\n",
              "8   47386184821       2.0   -45.480280    -33.416534   78.938129\n",
              "9   47386227441       2.0   -26.726782   -107.707639  134.474227\n",
              "10  47386489301       1.0  -248.231221    241.441644    6.823592\n",
              "11  47386496301       1.0  -257.821194    215.017403   42.845801\n",
              "12  47386582181       2.0  -303.021013     57.972000  245.083439\n",
              "13  47386582311       2.0  -383.384179     73.611511  309.816812\n",
              "14  47386675521       2.0  -349.663751     18.540242  331.164658\n",
              "15  47386858431       0.0   134.549230    -58.849803  -75.658888\n",
              "16  47386858681       0.0    56.552566    -52.425554   -4.084421\n",
              "17  47386859231       2.0   -13.049640    -60.084771   73.179789\n",
              "18  47386954741       0.0   324.890515   -209.497022 -115.344091\n",
              "19  47387007211       1.0  -256.566473    189.084735   67.517365"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c9db851f-a8cc-4270-839e-c2ecea2f8106\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>photo_id</th>\n",
              "      <th>S3_label</th>\n",
              "      <th>Prob_indoor</th>\n",
              "      <th>Prob_natural</th>\n",
              "      <th>Prob_urban</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>47386036511</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-146.067384</td>\n",
              "      <td>-80.056533</td>\n",
              "      <td>226.165605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>47386038251</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-184.546766</td>\n",
              "      <td>-62.174156</td>\n",
              "      <td>246.758862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>47386039291</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-113.993057</td>\n",
              "      <td>-65.118958</td>\n",
              "      <td>179.150011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>47386040661</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-152.126420</td>\n",
              "      <td>-56.406855</td>\n",
              "      <td>208.573240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>47386048321</td>\n",
              "      <td>0.0</td>\n",
              "      <td>240.147499</td>\n",
              "      <td>-147.099608</td>\n",
              "      <td>-93.008178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>47386064931</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-368.825431</td>\n",
              "      <td>124.437545</td>\n",
              "      <td>244.419923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>47386094261</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-238.672925</td>\n",
              "      <td>185.648652</td>\n",
              "      <td>53.072303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>47386176281</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.440025</td>\n",
              "      <td>-100.908386</td>\n",
              "      <td>99.508767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>47386184821</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-45.480280</td>\n",
              "      <td>-33.416534</td>\n",
              "      <td>78.938129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>47386227441</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-26.726782</td>\n",
              "      <td>-107.707639</td>\n",
              "      <td>134.474227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>47386489301</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-248.231221</td>\n",
              "      <td>241.441644</td>\n",
              "      <td>6.823592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>47386496301</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-257.821194</td>\n",
              "      <td>215.017403</td>\n",
              "      <td>42.845801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>47386582181</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-303.021013</td>\n",
              "      <td>57.972000</td>\n",
              "      <td>245.083439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>47386582311</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-383.384179</td>\n",
              "      <td>73.611511</td>\n",
              "      <td>309.816812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>47386675521</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-349.663751</td>\n",
              "      <td>18.540242</td>\n",
              "      <td>331.164658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>47386858431</td>\n",
              "      <td>0.0</td>\n",
              "      <td>134.549230</td>\n",
              "      <td>-58.849803</td>\n",
              "      <td>-75.658888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>47386858681</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56.552566</td>\n",
              "      <td>-52.425554</td>\n",
              "      <td>-4.084421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>47386859231</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-13.049640</td>\n",
              "      <td>-60.084771</td>\n",
              "      <td>73.179789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>47386954741</td>\n",
              "      <td>0.0</td>\n",
              "      <td>324.890515</td>\n",
              "      <td>-209.497022</td>\n",
              "      <td>-115.344091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>47387007211</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-256.566473</td>\n",
              "      <td>189.084735</td>\n",
              "      <td>67.517365</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9db851f-a8cc-4270-839e-c2ecea2f8106')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c9db851f-a8cc-4270-839e-c2ecea2f8106 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c9db851f-a8cc-4270-839e-c2ecea2f8106');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3d647b5b6504>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#final merge with the original csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mnew_df_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_10k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'photo_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'final DataFrame'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mnew_df_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   7948\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7950\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m   7951\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7952\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m ) -> \"DataFrame\":\n\u001b[0;32m---> 74\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;31m# to avoid incompatible dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_coerce_merge_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;31m# If argument passed to validate,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1163\u001b[0m                     \u001b[0minferred_right\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring_types\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minferred_left\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m                 ):\n\u001b[0;32m-> 1165\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;31m# datetimelikes must match exactly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#initialiaze the classifier\n",
        "scene_classifier = SceneClassifier(runtime='cpu')\n",
        "\n",
        "#list of the images with full path\n",
        "path = r'/content/drive/MyDrive/GeoEstimation/resources/images/new_data10k'\n",
        "path_list = os.listdir(path)\n",
        "path_list = [path+'/'+im for im in path_list if im[-3:]=='jpg']\n",
        "print('num of images is ' , len(path_list))\n",
        "\n",
        "#original file csv with images info\n",
        "data_10k = pd.read_csv(r'/content/drive/MyDrive/GeoEstimation/resources/images/final_dataset_10k.csv', sep=';')\n",
        "print('the origninal df of images info is')\n",
        "display(data_10k.head())\n",
        "\n",
        "#classification of the images, producing both S3_labels that probs\n",
        "places_prob, S3_labels = scene_classifier.process_images(path_list,b_size=128)\n",
        "print('num of triplette of probabilities is ' , len(places_prob))\n",
        "print('num of  S3_labels is ', len(S3_labels))\n",
        "\n",
        "#new file csv formation\n",
        "new_data = pd.DataFrame(data = np.asarray(places_prob), columns=['Prob_indoor','Prob_natural','Prob_urban'])\n",
        "S3_labels_data =  pd.DataFrame(data = np.asarray([S3_labels]).T ,columns=['S3_label'])\n",
        "images_name = [im[:-4] for im in os.listdir(path) if im[-3:]=='jpg']\n",
        "images_name_df = pd.DataFrame(data = images_name ,  columns=['photo_id'])\n",
        "\n",
        "#new df with the new information\n",
        "new_df = pd.concat([images_name_df,S3_labels_data,new_data],axis=1)\n",
        "print('the new data column we have now are')\n",
        "display(new_df.head(20))\n",
        "\n",
        "#final merge with the original csv\n",
        "data_10k['photo_id']=data_10k['photo_id'].apply(lambda x: str(x))\n",
        "new_df_full = data_10k.merge(right = new_df, on='photo_id')\n",
        "print('final DataFrame')\n",
        "new_df_full['photo_id'] = new_df_full['photo_id'].apply(lambda x: x+'.jpg') \n",
        "new_df_full.drop(new_df_full.columns[new_df_full.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
        "new_df_full.head()\n",
        "\n",
        "#save the new dataframe in a csv\n",
        "print(\"Let's save the results\")\n",
        "new_df_full.to_csv(path_or_buf=r'/content/drive/MyDrive/GeoEstimation/resources/images/data10k_places365.csv', index=False, sep=',')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "e583cc450fac4fd294c8076a1fbf5ede",
    "deepnote_persisted_session": {
      "createdAt": "2023-01-18T12:55:37.385Z"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "4fa9b9afc2baba5ccea1aafc34033c1dd8dcf2295c2dc2a369baeb32b0f17743"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6b33d04d11d4470cbfcf6326531e6fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e7931760b03424d897b5c635af54832",
              "IPY_MODEL_31bed04aa41b43a9ac14928426d3fc73",
              "IPY_MODEL_6e46a2b63ede4ebb937d9a48a14ef355"
            ],
            "layout": "IPY_MODEL_ec91d6421db149568f4707a854040869"
          }
        },
        "3e7931760b03424d897b5c635af54832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8973c26d36c3435c95d19ece007c8f81",
            "placeholder": "​",
            "style": "IPY_MODEL_68a2181ae5794a3a8b48c60895384986",
            "value": "100%"
          }
        },
        "31bed04aa41b43a9ac14928426d3fc73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2f0549a63f64823a434168c6b96eb3f",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56e9dc76e0384e52ba4cb2eadb17f151",
            "value": 102530333
          }
        },
        "6e46a2b63ede4ebb937d9a48a14ef355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c40c00c0f6648609f56e7c10022fea6",
            "placeholder": "​",
            "style": "IPY_MODEL_5ccff7a9383b403f9ddc1b6c29f490f8",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 297MB/s]"
          }
        },
        "ec91d6421db149568f4707a854040869": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8973c26d36c3435c95d19ece007c8f81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68a2181ae5794a3a8b48c60895384986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2f0549a63f64823a434168c6b96eb3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56e9dc76e0384e52ba4cb2eadb17f151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c40c00c0f6648609f56e7c10022fea6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ccff7a9383b403f9ddc1b6c29f490f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6298babe16614ae0bf724356680f9ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_190ed9a6ee42446db2a2d45506f61e7e",
              "IPY_MODEL_a59d401859a04cef9d41162904d728d7",
              "IPY_MODEL_1ac2579ce8f94562a74ae7f048885edc"
            ],
            "layout": "IPY_MODEL_8c5e8b9cc38e479e8d1d2c231cbf0ffa"
          }
        },
        "190ed9a6ee42446db2a2d45506f61e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f30295f9dba04eca89dddc062a3ed881",
            "placeholder": "​",
            "style": "IPY_MODEL_7edb1455eb8347288073c720eb8ebb95",
            "value": "100%"
          }
        },
        "a59d401859a04cef9d41162904d728d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_125227802eef4216b7d9264a3e994196",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e3bc8c827784295bef2fbe50424a816",
            "value": 170498071
          }
        },
        "1ac2579ce8f94562a74ae7f048885edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b595806ae0424fc6b4a3369ab5836cc7",
            "placeholder": "​",
            "style": "IPY_MODEL_67c610b42cff429382fbfa495010f482",
            "value": " 170498071/170498071 [00:03&lt;00:00, 84992213.51it/s]"
          }
        },
        "8c5e8b9cc38e479e8d1d2c231cbf0ffa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f30295f9dba04eca89dddc062a3ed881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7edb1455eb8347288073c720eb8ebb95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "125227802eef4216b7d9264a3e994196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e3bc8c827784295bef2fbe50424a816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b595806ae0424fc6b4a3369ab5836cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67c610b42cff429382fbfa495010f482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}