{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLeKGLqX_H4y"
      },
      "source": [
        "\n",
        "# Vision and Cognitive Systems - Project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GianmarcoLattaruolo/Vision_Project/blob/main/Vision_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q0q-cHAgX6g"
      },
      "source": [
        "# Preliminaries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BILxvIlegX6m"
      },
      "source": [
        "## Setting up the working space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmXSsJJKniqF"
      },
      "source": [
        "In this first cell we check if the notebook is runnig in Colab. In this case we need some additional work to set properly the environmet. We need also to mount our vision drive. In local machine instead we need to add the Geoestimation folder of our paper in the paths where python searches for libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSi1dIDk_H5R",
        "outputId": "c9db6b4d-d22e-4f15-d34b-5b094e803bde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "are we in Colab?: True\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m‚ú®üç∞‚ú® Everything looks OK!\n"
          ]
        }
      ],
      "source": [
        "# with this line we can check if we are in colab or not\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "print(\"are we in Colab?:\",in_colab)\n",
        "\n",
        "cwd = Path(os.getcwd())\n",
        "if in_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !pip install -q condacolab\n",
        "    import condacolab\n",
        "    condacolab.install()\n",
        "    os.chdir(cwd /'drive'/'MyDrive'/'GeoEstimation')\n",
        "else:\n",
        "    #our defult wd in local should be Vision_Project\n",
        "    if str(cwd)[-14:] == 'Vision_Project':\n",
        "        os.chdir(cwd / 'GeoEstimation')\n",
        "    sys.path.append(cwd / 'GeoEstimation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6EuxN1R_Zwi",
        "outputId": "57516044-99ec-4b09-e31a-44c13a89f11f"
      },
      "outputs": [],
      "source": [
        "# this cell takes a lot of time on colab!\n",
        "import sys\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "if in_colab:\n",
        "    import condacolab\n",
        "    condacolab.check()\n",
        "    import os\n",
        "    os.chdir(r'/content/drive/MyDrive/GeoEstimation')\n",
        "    print(os.getcwd())\n",
        "    !conda env update -n base -f environment.yml\n",
        "    # The following is ridiculous, I know, but it seems to work\n",
        "    !pip uninstall torchtext\n",
        "    !pip install torchtext==0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSyJGMw23UYv"
      },
      "source": [
        "In theory we need to install some specific packages with certain version to account for the original environment in which the paper results were obtained:\n",
        "```\n",
        "  - python=3.8\n",
        "  - msgpack-python=1.0.0\n",
        "  - pandas=1.1.5\n",
        "  - yaml=0.2.5\n",
        "  - tqdm=4.50\n",
        "  - cudatoolkit=10.2\n",
        "  - pytorch=1.6\n",
        "  - torchvision=0.7\n",
        "  - pytorch-lightning=1.0.1\n",
        "  - pip\n",
        "  - pip:\n",
        "    - s2sphere==0.2.5\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0oiwBBngX7A"
      },
      "source": [
        "## A transfer learning example: the strenght of pytorch-lightning\n",
        "\n",
        "Here we want to show in a nutshell the transfer learning approach from a pretrained model using both standard code and pytorch-lightning, to highlight the differences. Moreover we are going to load the same pretrained model (ResNet50) used by the authors as backbone to develop their ML model. For seek of semplicity we are going to re-train this model on the Cifar10. First let's see the classic torch approach:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6b33d04d11d4470cbfcf6326531e6fa1",
            "3e7931760b03424d897b5c635af54832",
            "31bed04aa41b43a9ac14928426d3fc73",
            "6e46a2b63ede4ebb937d9a48a14ef355",
            "ec91d6421db149568f4707a854040869",
            "8973c26d36c3435c95d19ece007c8f81",
            "68a2181ae5794a3a8b48c60895384986",
            "f2f0549a63f64823a434168c6b96eb3f",
            "56e9dc76e0384e52ba4cb2eadb17f151",
            "6c40c00c0f6648609f56e7c10022fea6",
            "5ccff7a9383b403f9ddc1b6c29f490f8",
            "6298babe16614ae0bf724356680f9ae0",
            "190ed9a6ee42446db2a2d45506f61e7e",
            "a59d401859a04cef9d41162904d728d7",
            "1ac2579ce8f94562a74ae7f048885edc",
            "8c5e8b9cc38e479e8d1d2c231cbf0ffa",
            "f30295f9dba04eca89dddc062a3ed881",
            "7edb1455eb8347288073c720eb8ebb95",
            "125227802eef4216b7d9264a3e994196",
            "5e3bc8c827784295bef2fbe50424a816",
            "b595806ae0424fc6b4a3369ab5836cc7",
            "67c610b42cff429382fbfa495010f482"
          ]
        },
        "id": "ia68yVpogX7F",
        "outputId": "6cb758a1-4f96-4208-e3f9-dd15bb42ff7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b33d04d11d4470cbfcf6326531e6fa1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6298babe16614ae0bf724356680f9ae0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "Epoch 0\n",
            "3.1930480003356934\n",
            "3.123041868209839\n",
            "3.0640506744384766\n",
            "3.0658154487609863\n",
            "3.264195442199707\n",
            "3.7586264610290527\n",
            "3.468670606613159\n",
            "3.4459760189056396\n",
            "3.164868116378784\n",
            "3.2048826217651367\n",
            "Epoch 1\n",
            "3.4092202186584473\n",
            "3.2618446350097656\n",
            "3.543487787246704\n",
            "2.9497785568237305\n",
            "3.0474600791931152\n",
            "3.175663709640503\n",
            "2.6673035621643066\n",
            "3.1997504234313965\n",
            "2.9967129230499268\n",
            "3.018550395965576\n",
            "Epoch 2\n",
            "3.0200114250183105\n",
            "3.6485610008239746\n",
            "2.8175318241119385\n",
            "3.3879618644714355\n",
            "2.957050323486328\n",
            "2.942509889602661\n",
            "3.0313799381256104\n",
            "2.8381459712982178\n",
            "2.9101810455322266\n",
            "2.69409441947937\n",
            "Epoch 3\n",
            "3.0937843322753906\n",
            "2.800339698791504\n",
            "2.7473137378692627\n",
            "2.8794608116149902\n",
            "2.829761266708374\n",
            "2.7380940914154053\n",
            "2.991612434387207\n",
            "2.689542770385742\n",
            "2.536581516265869\n",
            "3.066215991973877\n",
            "Epoch 4\n",
            "2.942972421646118\n",
            "2.8620858192443848\n",
            "2.899718999862671\n",
            "2.8575029373168945\n",
            "2.734931707382202\n",
            "2.764251708984375\n",
            "2.7208406925201416\n",
            "2.7029600143432617\n",
            "2.576611042022705\n",
            "2.772939920425415\n",
            "Epoch 5\n",
            "2.8225460052490234\n",
            "2.631972074508667\n",
            "2.8054146766662598\n",
            "2.5750010013580322\n",
            "2.774996280670166\n",
            "2.6839115619659424\n",
            "2.857142686843872\n",
            "2.3621490001678467\n",
            "2.97297739982605\n",
            "2.690294027328491\n",
            "Epoch 6\n",
            "2.7450766563415527\n",
            "3.1124308109283447\n",
            "2.525341510772705\n",
            "2.5314621925354004\n",
            "2.88797664642334\n",
            "2.4406018257141113\n",
            "2.2911229133605957\n",
            "2.389775514602661\n",
            "2.615004062652588\n",
            "2.8636035919189453\n",
            "Epoch 7\n",
            "2.6630327701568604\n",
            "2.590579032897949\n",
            "2.5763955116271973\n",
            "2.500095844268799\n",
            "2.5076839923858643\n",
            "2.598517656326294\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d719591ce19d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m               \u001b[0;31m#(b, 3, 32, 32) -> (b, 1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m               \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m           \u001b[0;31m# (b, 1000) -> (b, 10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#libraries\n",
        "from torchvision import models\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import softmax, cross_entropy\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "#check for GPU\n",
        "want_gpu = True\n",
        "if want_gpu and torch.cuda.is_available():\n",
        "    gpu = 1\n",
        "else:\n",
        "    gpu = None\n",
        "\n",
        "#download the pretrained model\n",
        "backbone = models.resnet50(pretrained = True)\n",
        "\n",
        "#download and normalize the CIFAR10 dataset\n",
        "normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                 std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
        "cf10_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "cifar_10 = CIFAR10('.',train=True, download = True, transform=cf10_transforms) \n",
        "\n",
        "'''#train, validation and test split (we have to set the seed)\n",
        "train_data, val_data, test_data = random_split(cifar_10, [40000, 10000, 10000] )'''\n",
        "\n",
        "#prepare the batches\n",
        "train_loader = DataLoader(cifar_10, batch_size=32, shuffle=True)\n",
        "'''val_loader = DataLoader(val_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=True)'''\n",
        "\n",
        "# We add to the last layer with a fully connected one to match our number of classes (=10):\n",
        "# We treat the outputs of resnet as high level features (we could use them with any classifier instead of a FC)\n",
        "finetune_layer = torch.nn.Linear(backbone.fc.out_features, 10) \n",
        "#finetune_layer = torch.nn.Linear(backbone.fc.in_features, 10) is for REPLACE THE LAST LAYER\n",
        "\n",
        "#define the optimizer\n",
        "optimizer = Adam(finetune_layer.parameters(), lr = 1e-4)\n",
        "\n",
        "#we set a limit for the number of batches in each epoch, since we are not interesting in training proper this model\n",
        "limit_train_batches = 10\n",
        "\n",
        "#training\n",
        "for epoch in range(10):\n",
        "    print(f'Epoch {epoch}')\n",
        "    for i,batch in enumerate(train_loader):\n",
        "      if i<limit_train_batches:\n",
        "\n",
        "          x, y = batch\n",
        "          #we do not waste memory recording the gradient on the backbone\n",
        "          with torch.no_grad():\n",
        "              #(b, 3, 32, 32) -> (b, 1000)\n",
        "              features = backbone(x)\n",
        "\n",
        "          # (b, 1000) -> (b, 10)\n",
        "          preds = finetune_layer(features)\n",
        "          loss = cross_entropy(preds, y)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "          print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Rf6CfUHgX7L"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.metrics.functional import accuracy\n",
        "\n",
        "class ImageClassifier(pl.LightningModule):\n",
        "    def __init__(self, num_classes=10 , lr = 1e-3):\n",
        "        super().__init__()\n",
        "        #this setting save as the time to define an attribute for each hyperparameter --> self.hparams.<parameter>\n",
        "        self.save_hyperparameters() #Pytorch-lightning trick!\n",
        "        self.backbone = models.resnet50(pretrained = True)\n",
        "        self.finetune_layer = torch.nn.Linear(backbone.fc.out_features, num_classes)\n",
        "\n",
        "    def training_step(self, batch, batch_idx): #these methods are standard methods in LightningModule\n",
        "        x, y = batch\n",
        "\n",
        "        #we decide whether to freeze the backbone or not on the base of the number of epochs\n",
        "        if self.trainer.current_epoch < 10:\n",
        "            with torch.no_grad():\n",
        "                #(b, 3, 32, 32) -> (b, 1000)\n",
        "                features = self.backbone(x)\n",
        "        else:\n",
        "            features = self.backbone(x)\n",
        "\n",
        "        # (b, 1000) -> (b, 10)\n",
        "        preds = self.finetune_layer(features)\n",
        "        loss = cross_entropy(preds, y)\n",
        "        #we don't need anymore loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
        "        self.log('train_loss', loss) # we will see later this method of LightningModule\n",
        "        self.log('train_loss', accuracy(preds, y))\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # (b, 1000) -> (b, 10)\n",
        "        preds = self.finetune_layer(features)\n",
        "        loss = cross_entropy(preds, y)\n",
        "        #we don't need anymore loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
        "        self.log('val_loss', loss) # we will see later this method of LightningModule\n",
        "        self.log('val_loss', accuracy(preds, y))\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = Adam(self.parameters(), lr =self.hparams.lr) \n",
        "        #we can safely pass all the parameters since in the backbone we are not computing the gradient\n",
        "        return optimizer\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv9Yxw-AgX7R"
      },
      "source": [
        "At this point we have  very flexible object ```ImageClassifier```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1ig2I1hgX7U"
      },
      "outputs": [],
      "source": [
        "from pl_bolts.datamodules import CIFAR10DataModule\n",
        "\n",
        "#Bolts save us the time of train, vla, test split and using 3 different torch.Dataloader for each of them\n",
        "dm = CIFAR10DataModule('.') \n",
        "\n",
        "classifier = ImageClassifier()\n",
        "logger = pl.loggers.TensorBoardLogger(name = f'pretrained model 1', save_dir = 'lightning_logs')\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs = 2, # set the number of epochs if <1000 (=defult)\n",
        "    progress_bar_refresh_rate = 20, \n",
        "    logger = logger,\n",
        "    #gpus=1, \n",
        "    limit_train_batches = 50,\n",
        "    #limit_val_batches = 2,\n",
        "    #check_val_every_n_epoch = 5\n",
        "    #fast_dev_run=True # add this to have a fast chech of bugs\n",
        "    ) \n",
        "trainer.fit(classifier, dm) #we can use the normal train_loader we defined previously\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkDLLwMegX7X"
      },
      "source": [
        "We can use this very nice [tool](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.loggers.tensorboard.html#module-pytorch_lightning.loggers.tensorboard) form Pytorch-Lighting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1EhLmc7gX7Z"
      },
      "outputs": [],
      "source": [
        "# start tensorboard\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgm93PqcgX7b"
      },
      "source": [
        "### Self-supervised transfer learning with Lightning\n",
        "\n",
        "PyTorch Lightning implementation of SwAV adapted from the [official implementation](https://arxiv.org/abs/2006.09882), whose authors used the same pretrained model (ResNet50 trained on ImageNet). We can simply import this model from Lightning-Bolt and define a class very similar to the previous one for our classifer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FC_NgnkgX7c"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import softmax, cross_entropy\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import random_split\n",
        "import pytorch_lightning as pl\n",
        "from pl_bolts.models.self_supervised import SwAV\n",
        "from pl_bolts.datamodules import CIFAR10DataModule\n",
        "\n",
        "#Bolts save us the time of train, vla, test split and using 3 different torch.Dataloader for each of them\n",
        "dm = CIFAR10DataModule('.') \n",
        "\n",
        "#weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/swav/bolts_swav_imagenet/swav_imagenet.ckpt'\n",
        "#weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/swav/swav_imagenet/swav_imagenet.pth.tar'\n",
        "swav = SwAV.load_from_checkpoint(r'C:\\Users\\latta\\.cache\\torch\\hub\\checkpoints\\swav_imagenet.pth.tar', strict=False)\n",
        "\n",
        "class SSLImageClassifier(pl.LightningModule):\n",
        "    def __init__(self, num_classes=10 , lr = 1e-3):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters() \n",
        "        self.backbone = swav.model #model pretrained on ImageNet without labels\n",
        "        self.finetune_layer = torch.nn.Linear(3000, num_classes)\n",
        "\n",
        "    def training_step(self, batch, batch_idx): #these methods are standard methods in \n",
        "        x, y = batch\n",
        "\n",
        "        #we decide whether to freeze the backbone or not on the base of the number of epochs\n",
        "        if self.trainer.current_epoch < 10:\n",
        "            with torch.no_grad():\n",
        "                #(b, 3, 32, 32) -> (b, 1000)\n",
        "                (f1, f2) = self.backbone(x)\n",
        "                features = f2\n",
        "        else:\n",
        "            (f1, f2) = self.backbone(x)\n",
        "            features = f2\n",
        "\n",
        "        # (b, 1000) -> (b, 10)\n",
        "        preds = self.finetune_layer(features)\n",
        "        loss = cross_entropy(preds, y)\n",
        "        #we don't need anymore loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
        "        self.log('train_loss', loss) # we will see later this method of LightningModule\n",
        "        self.log('train_loss', accuracy(preds, y))\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx): #these methods are standard methods in \n",
        "        x, y = batch\n",
        "        \n",
        "        (f1, f2) = self.backbone(x)\n",
        "        features = f2\n",
        "\n",
        "        # (b, 1000) -> (b, 10)\n",
        "        preds = self.finetune_layer(features)\n",
        "        loss = cross_entropy(preds, y)\n",
        "        #we don't need anymore loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
        "        self.log('val_loss', loss) # we will see later this method of LightningModule\n",
        "        self.log('val_loss', accuracy(preds, y))\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = Adam(self.parameters(), lr =self.hparams.lr) \n",
        "        #we can safely pass all the parameters since in the backbone we are not computing the gradient\n",
        "        return optimizer\n",
        "\n",
        "'''backbone = models.resnet50(pretrained = True)\n",
        "finetune_layer = torch.nn.Linear(backbone.fc.out_features, 10) \n",
        "#define the optimizer\n",
        "optimizer = Adam(finetune_layer.parameters(), lr = 1e-4)\n",
        "'''\n",
        "ssl_classifier = SSLImageClassifier()\n",
        "logger = pl.loggers.TensorBoardLogger(name = f'pretrained model 2 (self superised)', save_dir = 'lightning_logs')\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs = 2,\n",
        "    #progress_bar_refresh_rate = 20, \n",
        "    gpus=0, \n",
        "    limit_train_batches = 50#, \n",
        "    #fast_dev_run=True # add this to have a fast chech of bugs\n",
        "    ) \n",
        "trainer.fit(ssl_classifier, dm) #we can use the normal train_loader we defined previously"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d5LUBSEgX7i"
      },
      "outputs": [],
      "source": [
        "# start tensorboard\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4BF4QseoP35"
      },
      "source": [
        "## Reproduce paper results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwz47JYx_H5b"
      },
      "source": [
        "To begin we try to reproduce the paper results on their test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay_jd1lT_H5d"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from math import ceil\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from classification.train_base import MultiPartitioningClassifier # class defining our model\n",
        "from classification.dataset import FiveCropImageDataset # class for preparing the images before giving them to the NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiYqpWSn_H5i"
      },
      "source": [
        "### Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s09ZTd3_H5k"
      },
      "outputs": [],
      "source": [
        "# where model's params and hyperparams are saved\n",
        "checkpoint = \"models/base_M/epoch=014-val_loss=18.4833.ckpt\"\n",
        "hparams = \"models/base_M/hparams.yaml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IsHtRs6_H5m"
      },
      "outputs": [],
      "source": [
        "# load_from_checkpoint is a static method from pytorch lightning, inherited by MultiPartitioningClassifier\n",
        "# it permits to load a model previously saved, in the form of a checkpoint file, and one with hyperparameters\n",
        "# MultiPartitioningClassifier is the class defining our model\n",
        "model = MultiPartitioningClassifier.load_from_checkpoint(\n",
        "    checkpoint_path=checkpoint,\n",
        "    hparams_file=hparams,\n",
        "    map_location=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqaPzJCygX7s"
      },
      "outputs": [],
      "source": [
        "type(pl.LightningModule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lANl7PDx_H5o"
      },
      "outputs": [],
      "source": [
        "#to allow GPU\n",
        "want_gpu = True\n",
        "if want_gpu and torch.cuda.is_available():\n",
        "    gpu = 1\n",
        "else:\n",
        "    gpu = None\n",
        "\n",
        "# the class Trainer from pythorch lightining is the one responsible for training a deep NN\n",
        "# it can initialize the model, run forward and backward passes, optimize, print stats, early stop...\n",
        "wanted_precision = 32 #16 for half precision (how many bits for each number)\n",
        "trainer = pl.Trainer(gpus=gpu, precision=wanted_precision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csYOzf4r_H5p"
      },
      "source": [
        "### Load and initialize the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdLRVDHC_H5q"
      },
      "outputs": [],
      "source": [
        "# where images are saved\n",
        "image_dir = \"resources/images/im2gps\"\n",
        "meta_csv = \"resources/images/im2gps_places365.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5WRFwx0gX74"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "first_csv = pd.read_csv(meta_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf0Rm1h8_H5r"
      },
      "outputs": [],
      "source": [
        "#FiveCropImageDataset is the class for preparing the images before giving them to the NN\n",
        "# in particular, it creates five different crops for every image\n",
        "dataset = FiveCropImageDataset(meta_csv, image_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNIX_tu2_H5s"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "                    dataset,\n",
        "                    batch_size=ceil(batch_size / 5),  #you divide by 5 because for each image you generate 5 different crops\n",
        "                    shuffle=False,\n",
        "                    num_workers=4 #number ot threads used for parallelism (cores of CPU?)\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQycvZEH_H5s"
      },
      "source": [
        "### Run the model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwfspFbj_H5t"
      },
      "outputs": [],
      "source": [
        "results = trainer.test(model, test_dataloaders=dataloader, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu5nzy7B_H5u"
      },
      "source": [
        "### Look at the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OHtm_1M_H5u"
      },
      "outputs": [],
      "source": [
        "# formatting results into a pandas dataframe\n",
        "df = pd.DataFrame(results[0]).T\n",
        "#df[\"dataset\"] = image_dir\n",
        "df[\"partitioning\"] = df.index\n",
        "df[\"partitioning\"] = df[\"partitioning\"].apply(lambda x: x.split(\"/\")[-1])\n",
        "df.set_index(keys=[\"partitioning\"], inplace=True) #keys=[\"dataset\", \"partitioning\"] in case\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6vSJgR0_H5v"
      },
      "outputs": [],
      "source": [
        "# to save the dataframe on a csv file\n",
        "fout = 'test_results.csv'\n",
        "df.to_csv(fout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2g9datY1blm"
      },
      "outputs": [],
      "source": [
        "os.chdir(r'/content/drive/MyDrive/GeoEstimation/resources/images/im2gps')\n",
        "print(len(os.listdir()))\n",
        "os.chdir(r'/content/drive/MyDrive/GeoEstimation')\n",
        "print(os.getcwd())\n",
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "# Output would be True if Pytorch is using GPU otherwise it would be False.\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B3kpB3fnTzR",
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#libraries to import\n",
        "#known\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import torchvision\n",
        "import torch\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "import sys\n",
        "import time\n",
        "from math import ceil\n",
        "\n",
        "\n",
        "\n",
        "#Unknown\n",
        "from typing import Union\n",
        "from io import BytesIO\n",
        "import random\n",
        "from argparse import Namespace, ArgumentParser\n",
        "from pathlib import Path\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "import requests\n",
        "import logging\n",
        "import json\n",
        "import yaml\n",
        "from tqdm.auto import tqdm\n",
        "#from classification.train_base import MultiPartitioningClassifier\n",
        "#from classification.dataset import FiveCropImageDataset\n",
        "\n",
        "#to divide\n",
        "from classification import utils_global\n",
        "from classification.s2_utils import Partitioning, Hierarchy\n",
        "from classification.dataset import MsgPackIterableDatasetMultiTargetWithDynLabels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFcvIDD5nTzr"
      },
      "source": [
        "The main link and paper that we need to follow is [this](https://github.com/TIBHannover/GeoEstimation) and [this](https://github.com/TIBHannover/GeoEstimation/releases/) for the pretrained models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e56Y49vfnTzr"
      },
      "source": [
        "Davide ha trovato questo che forse √® meglio [kaggle](https://www.kaggle.com/code/habedi/inspect-the-dataset/data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0OQXXljgX8T"
      },
      "source": [
        "## Our dataset\n",
        "\n",
        "We have downloaded a new 10k dataset with gps coordinates. We need the labels for the scenes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q-pBnlfClJDN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import importlib\n",
        "imported_module = importlib.import_module(\"scene_classification\")\n",
        "importlib.reload(imported_module)\n",
        "import scene_classification\n",
        "from scene_classification import SceneClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wnCXTaVggX8V",
        "outputId": "ca6c9502-e8db-474d-d359-0d934af2ab83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading scene hierarchy ...\n",
            "num of images is  9446\n",
            "the origninal df of images info is\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-905f0d3f-11cb-40e0-ad9e-7512d92d6615\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>photo_id</th>\n",
              "      <th>owner</th>\n",
              "      <th>gender</th>\n",
              "      <th>occupation</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>faves</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>u_city</th>\n",
              "      <th>u_country</th>\n",
              "      <th>taken</th>\n",
              "      <th>weather</th>\n",
              "      <th>season</th>\n",
              "      <th>daytime</th>\n",
              "      <th>base_url</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>17271526139</td>\n",
              "      <td>130418712@N05</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Rio Trejo</td>\n",
              "      <td>Son numerosos los rios y arroyos que discurren...</td>\n",
              "      <td>701.0</td>\n",
              "      <td>36,861544</td>\n",
              "      <td>-5,177747</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-04-26 17:11:11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>https://www.flickr.com/photos/130418712@N05/17...</td>\n",
              "      <td>https://live.staticflickr.com/65535/1727152613...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>17776887679</td>\n",
              "      <td>55101137@N02</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-05-13-022FD PH-XRD</td>\n",
              "      <td>&lt;u&gt;&lt;b&gt;Aircraft Type - Registration - (c/n)&lt;/b&gt;...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51,463766</td>\n",
              "      <td>5,392935</td>\n",
              "      <td>Bodmin</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>2015-05-13 00:00:22</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>https://www.flickr.com/photos/55101137@N02/177...</td>\n",
              "      <td>https://live.staticflickr.com/5335/17776887679...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>17898331633</td>\n",
              "      <td>55101137@N02</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-05-17-022FD OO-GWA</td>\n",
              "      <td>&lt;u&gt;&lt;b&gt;Aircraft Type - Registration - (c/n)&lt;/b&gt;...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>51,190492</td>\n",
              "      <td>4,453765</td>\n",
              "      <td>Bodmin</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>2015-05-17 00:00:22</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>https://www.flickr.com/photos/55101137@N02/178...</td>\n",
              "      <td>https://live.staticflickr.com/525/17898331633_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>17940239919</td>\n",
              "      <td>55101137@N02</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-05-14-020FD D-1553</td>\n",
              "      <td>&lt;u&gt;&lt;b&gt;Aircraft Type - Registration - (c/n)&lt;/b&gt;...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>51,326247</td>\n",
              "      <td>6,085953</td>\n",
              "      <td>Bodmin</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>2015-05-14 00:00:20</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>https://www.flickr.com/photos/55101137@N02/179...</td>\n",
              "      <td>https://live.staticflickr.com/8860/17940239919...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>17963122505</td>\n",
              "      <td>55101137@N02</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-05-13-025FD EI-DLI</td>\n",
              "      <td>&lt;u&gt;&lt;b&gt;Aircraft Type - Registration - (c/n)&lt;/b&gt;...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>51,463766</td>\n",
              "      <td>5,392935</td>\n",
              "      <td>Bodmin</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>2015-05-13 00:00:25</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>https://www.flickr.com/photos/55101137@N02/179...</td>\n",
              "      <td>https://live.staticflickr.com/5457/17963122505...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-905f0d3f-11cb-40e0-ad9e-7512d92d6615')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-905f0d3f-11cb-40e0-ad9e-7512d92d6615 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-905f0d3f-11cb-40e0-ad9e-7512d92d6615');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0     photo_id          owner  gender occupation  \\\n",
              "0           0  17271526139  130418712@N05     1.0        NaN   \n",
              "1           1  17776887679   55101137@N02     1.0        NaN   \n",
              "2           2  17898331633   55101137@N02     1.0        NaN   \n",
              "3           3  17940239919   55101137@N02     1.0        NaN   \n",
              "4           4  17963122505   55101137@N02     1.0        NaN   \n",
              "\n",
              "                     title                                        description  \\\n",
              "0                Rio Trejo  Son numerosos los rios y arroyos que discurren...   \n",
              "1  2015-05-13-022FD PH-XRD  <u><b>Aircraft Type - Registration - (c/n)</b>...   \n",
              "2  2015-05-17-022FD OO-GWA  <u><b>Aircraft Type - Registration - (c/n)</b>...   \n",
              "3  2015-05-14-020FD D-1553  <u><b>Aircraft Type - Registration - (c/n)</b>...   \n",
              "4  2015-05-13-025FD EI-DLI  <u><b>Aircraft Type - Registration - (c/n)</b>...   \n",
              "\n",
              "   faves        lat        lon  u_city       u_country                taken  \\\n",
              "0  701.0  36,861544  -5,177747     NaN             NaN  2015-04-26 17:11:11   \n",
              "1    1.0  51,463766   5,392935  Bodmin  United Kingdom  2015-05-13 00:00:22   \n",
              "2    2.0  51,190492   4,453765  Bodmin  United Kingdom  2015-05-17 00:00:22   \n",
              "3    0.0  51,326247   6,085953  Bodmin  United Kingdom  2015-05-14 00:00:20   \n",
              "4    2.0  51,463766   5,392935  Bodmin  United Kingdom  2015-05-13 00:00:25   \n",
              "\n",
              "   weather  season  daytime  \\\n",
              "0      NaN     1.0      2.0   \n",
              "1      9.0     1.0      3.0   \n",
              "2      9.0     1.0      3.0   \n",
              "3      9.0     1.0      3.0   \n",
              "4      9.0     1.0      3.0   \n",
              "\n",
              "                                            base_url  \\\n",
              "0  https://www.flickr.com/photos/130418712@N05/17...   \n",
              "1  https://www.flickr.com/photos/55101137@N02/177...   \n",
              "2  https://www.flickr.com/photos/55101137@N02/178...   \n",
              "3  https://www.flickr.com/photos/55101137@N02/179...   \n",
              "4  https://www.flickr.com/photos/55101137@N02/179...   \n",
              "\n",
              "                                                 url  \n",
              "0  https://live.staticflickr.com/65535/1727152613...  \n",
              "1  https://live.staticflickr.com/5335/17776887679...  \n",
              "2  https://live.staticflickr.com/525/17898331633_...  \n",
              "3  https://live.staticflickr.com/8860/17940239919...  \n",
              "4  https://live.staticflickr.com/5457/17963122505...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have 74 batches\n",
            "We are at batch  1\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  2\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  3\n",
            "this batch has size  torch.Size([127, 3, 256, 256])\n",
            "We are at batch  4\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  5\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  6\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  7\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  8\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  9\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  10\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  11\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  12\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  13\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  14\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  15\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  16\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  17\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  18\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  19\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  20\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  21\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  22\n",
            "this batch has size  torch.Size([127, 3, 256, 256])\n",
            "We are at batch  23\n",
            "this batch has size  torch.Size([127, 3, 256, 256])\n",
            "We are at batch  24\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  25\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  26\n",
            "this batch has size  torch.Size([127, 3, 256, 256])\n",
            "We are at batch  27\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  28\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  29\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  30\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  31\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  32\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  33\n",
            "this batch has size  torch.Size([127, 3, 256, 256])\n",
            "We are at batch  34\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  35\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n",
            "We are at batch  36\n",
            "this batch has size  torch.Size([128, 3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#initialiaze the classifier\n",
        "scene_classifier = SceneClassifier(runtime='cpu')\n",
        "\n",
        "#list of the images with full path\n",
        "path = r'/content/drive/MyDrive/GeoEstimation/resources/images/new_data10k'\n",
        "path_list = os.listdir(path)\n",
        "path_list = [path+'/'+im for im in path_list if im[-3:]=='jpg']\n",
        "print('num of images is ' , len(path_list))\n",
        "\n",
        "#original file csv with images info\n",
        "data_10k = pd.read_csv(r'/content/drive/MyDrive/GeoEstimation/resources/images/final_dataset_10k.csv', sep=';')\n",
        "print('the origninal df of images info is')\n",
        "display(data_10k.head())\n",
        "\n",
        "#classification of the images, producing both S3_labels that probs\n",
        "places_prob, S3_labels = scene_classifier.process_images(path_list,b_size=128)\n",
        "print('num of triplette of probabilities is ' , len(places_prob))\n",
        "print('num of  S3_labels is ', len(S3_labels))\n",
        "\n",
        "#new file csv formation\n",
        "new_data = pd.DataFrame(data = np.asarray(places_prob), columns=['Prob_indoor','Prob_natural','Prob_urban'])\n",
        "S3_labels_data =  pd.DataFrame(data = np.asarray([S3_labels]).T ,columns=['S3_label'])\n",
        "images_name = [im[:-4] for im in os.listdir(path) if im[-3:]=='jpg']\n",
        "images_name_df = pd.DataFrame(data = images_name ,  columns=['photo_id'])\n",
        "\n",
        "#new df with the new information\n",
        "new_df = pd.concat([images_name_df,S3_labels_data,new_data],axis=1)\n",
        "print('the new data column we have now are')\n",
        "display(new_df.head(20))\n",
        "\n",
        "#final merge with the original csv\n",
        "new_df_full = data_10k.merge(right = new_df, on='photo_id')\n",
        "print('final DataFrame')\n",
        "new_df_full.head()\n",
        "\n",
        "#save the new dataframe in a csv\n",
        "print(\"Let's save the results\")\n",
        "new_df_full.to_csv(path_or_buf=r'/content/drive/MyDrive/GeoEstimation/resources/images/data10k_places365.csv', sep=',')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "e583cc450fac4fd294c8076a1fbf5ede",
    "deepnote_persisted_session": {
      "createdAt": "2023-01-18T12:55:37.385Z"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "4fa9b9afc2baba5ccea1aafc34033c1dd8dcf2295c2dc2a369baeb32b0f17743"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "125227802eef4216b7d9264a3e994196": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "190ed9a6ee42446db2a2d45506f61e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f30295f9dba04eca89dddc062a3ed881",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7edb1455eb8347288073c720eb8ebb95",
            "value": "100%"
          }
        },
        "1ac2579ce8f94562a74ae7f048885edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b595806ae0424fc6b4a3369ab5836cc7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_67c610b42cff429382fbfa495010f482",
            "value": " 170498071/170498071 [00:03&lt;00:00, 84992213.51it/s]"
          }
        },
        "31bed04aa41b43a9ac14928426d3fc73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2f0549a63f64823a434168c6b96eb3f",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56e9dc76e0384e52ba4cb2eadb17f151",
            "value": 102530333
          }
        },
        "3e7931760b03424d897b5c635af54832": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8973c26d36c3435c95d19ece007c8f81",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_68a2181ae5794a3a8b48c60895384986",
            "value": "100%"
          }
        },
        "56e9dc76e0384e52ba4cb2eadb17f151": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5ccff7a9383b403f9ddc1b6c29f490f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e3bc8c827784295bef2fbe50424a816": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6298babe16614ae0bf724356680f9ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_190ed9a6ee42446db2a2d45506f61e7e",
              "IPY_MODEL_a59d401859a04cef9d41162904d728d7",
              "IPY_MODEL_1ac2579ce8f94562a74ae7f048885edc"
            ],
            "layout": "IPY_MODEL_8c5e8b9cc38e479e8d1d2c231cbf0ffa"
          }
        },
        "67c610b42cff429382fbfa495010f482": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68a2181ae5794a3a8b48c60895384986": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b33d04d11d4470cbfcf6326531e6fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e7931760b03424d897b5c635af54832",
              "IPY_MODEL_31bed04aa41b43a9ac14928426d3fc73",
              "IPY_MODEL_6e46a2b63ede4ebb937d9a48a14ef355"
            ],
            "layout": "IPY_MODEL_ec91d6421db149568f4707a854040869"
          }
        },
        "6c40c00c0f6648609f56e7c10022fea6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e46a2b63ede4ebb937d9a48a14ef355": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c40c00c0f6648609f56e7c10022fea6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5ccff7a9383b403f9ddc1b6c29f490f8",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 297MB/s]"
          }
        },
        "7edb1455eb8347288073c720eb8ebb95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8973c26d36c3435c95d19ece007c8f81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c5e8b9cc38e479e8d1d2c231cbf0ffa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a59d401859a04cef9d41162904d728d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_125227802eef4216b7d9264a3e994196",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e3bc8c827784295bef2fbe50424a816",
            "value": 170498071
          }
        },
        "b595806ae0424fc6b4a3369ab5836cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec91d6421db149568f4707a854040869": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2f0549a63f64823a434168c6b96eb3f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f30295f9dba04eca89dddc062a3ed881": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
