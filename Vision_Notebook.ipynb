{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4d112b5799924669bd63a025faac2662",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# Vision and Cognition Systems - Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#libraries to import\n",
    "#known\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import torchvision\n",
    "import torch\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "import sys\n",
    "import time\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "\n",
    "#Unknown\n",
    "from typing import Union\n",
    "from io import BytesIO\n",
    "import random\n",
    "from argparse import Namespace, ArgumentParser\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import requests\n",
    "import logging\n",
    "import json\n",
    "import yaml\n",
    "from tqdm.auto import tqdm\n",
    "#from classification.train_base import MultiPartitioningClassifier\n",
    "#from classification.dataset import FiveCropImageDataset\n",
    "\n",
    "#to divide\n",
    "from classification import utils_global\n",
    "from classification.s2_utils import Partitioning, Hierarchy\n",
    "from classification.dataset import MsgPackIterableDatasetMultiTargetWithDynLabels\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions in the first the main GitHub folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is from ```msgpack_viewer.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MsgPackIterableDataset(torch.utils.data.IterableDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str,\n",
    "        key_img_id: str = \"id\",\n",
    "        key_img_encoded: str = \"image\",\n",
    "        transformation=None,\n",
    "        shuffle=False,\n",
    "        cache_size=6 * 4096,\n",
    "    ):\n",
    "\n",
    "        super(MsgPackIterableDataset, self).__init__()\n",
    "        self.path = path\n",
    "        self.cache_size = cache_size\n",
    "        self.transformation = transformation\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = random.randint(1, 100)\n",
    "        self.key_img_id = key_img_id.encode(\"utf-8\")\n",
    "        self.key_img_encoded = key_img_encoded.encode(\"utf-8\")\n",
    "\n",
    "        if not isinstance(self.path, (list, set)):\n",
    "            self.path = [self.path]\n",
    "        \n",
    "        self.shards = self.__init_shards(self.path)\n",
    "\n",
    "    @staticmethod\n",
    "    def __init_shards(path: Union[str, Path]) -> list:\n",
    "        shards = []\n",
    "        for i, p in enumerate(path):\n",
    "            shards_re = r\"shard_(\\d+).msg\"\n",
    "            shards_index = [\n",
    "                int(re.match(shards_re, x).group(1))\n",
    "                for x in os.listdir(p)\n",
    "                if re.match(shards_re, x)\n",
    "            ]\n",
    "            shards.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"path_index\": i,\n",
    "                        \"path\": p,\n",
    "                        \"shard_index\": s,\n",
    "                        \"shard_path\": os.path.join(p, f\"shard_{s}.msg\"),\n",
    "                    }\n",
    "                    for s in shards_index\n",
    "                ]\n",
    "            )\n",
    "        if len(shards) == 0:\n",
    "            raise ValueError(\"No shards found\")\n",
    "        \n",
    "        return shards\n",
    "\n",
    "    def _process_sample(self, x):\n",
    "        # decode and initial resize if necessary\n",
    "        img = Image.open(BytesIO(x[self.key_img_encoded]))\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "\n",
    "        if img.width > 320 and img.height > 320:\n",
    "            img = torchvision.transforms.Resize(320)(img)\n",
    "\n",
    "        # apply all user specified image transformations\n",
    "        if self.transformation is not None:\n",
    "            img = self.transformation(img)\n",
    "        \n",
    "        _id = x[self.key_img_id].decode(\"utf-8\")\n",
    "        return img, _id\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        shard_indices = list(range(len(self.shards)))\n",
    "\n",
    "        if self.shuffle:\n",
    "            random.seed(self.seed)\n",
    "            random.shuffle(shard_indices)\n",
    "\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "\n",
    "        if worker_info is not None:\n",
    "\n",
    "            def split_list(alist, splits=1):\n",
    "                length = len(alist)\n",
    "                return [\n",
    "                    alist[i * length // splits : (i + 1) * length // splits]\n",
    "                    for i in range(splits)\n",
    "                ]\n",
    "\n",
    "            shard_indices_split = split_list(shard_indices, worker_info.num_workers)[\n",
    "                worker_info.id\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            shard_indices_split = shard_indices\n",
    "\n",
    "        cache = []\n",
    "\n",
    "        for shard_index in shard_indices_split:\n",
    "            shard = self.shards[shard_index]\n",
    "\n",
    "            with open(\n",
    "                os.path.join(shard[\"path\"], f\"shard_{shard['shard_index']}.msg\"), \"rb\"\n",
    "            ) as f:\n",
    "                unpacker = msgpack.Unpacker(\n",
    "                    f, max_buffer_size=1024 * 1024 * 1024, raw=True\n",
    "                )\n",
    "                for x in unpacker:\n",
    "                    if x is None:\n",
    "                        continue\n",
    "\n",
    "                    if len(cache) < self.cache_size:\n",
    "                        cache.append(x)\n",
    "\n",
    "                    if len(cache) == self.cache_size:\n",
    "\n",
    "                        if self.shuffle:\n",
    "                            random.shuffle(cache)\n",
    "                        while cache:\n",
    "                            yield self._process_sample(cache.pop())\n",
    "        if self.shuffle:\n",
    "            random.shuffle(cache)\n",
    "\n",
    "        while cache:\n",
    "            yield self._process_sample(cache.pop())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args = argparse.ArgumentParser()\n",
    "    args.add_argument(\"--data\", type=str, default=\"resources/images/mp16\")\n",
    "    args = args.parse_args()\n",
    "\n",
    "    tfm = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = MsgPackIterableDataset(path=args.data, transformation=tfm)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=1,\n",
    "            num_workers=6,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "    num_images = 0\n",
    "    for x, image_id in dataloader:\n",
    "        if num_images == 0:\n",
    "            print(x.shape, image_id)\n",
    "        num_images +=1\n",
    "    \n",
    "    print(f\"{num_images=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is from ```filter_by_downloaded_images.py``` that maybe we won't need since we will use a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    for dataset_type in [\"train\", \"val\"]:\n",
    "        with open(config[f\"{dataset_type}_label_mapping\"]) as f:\n",
    "            mapping = json.load(f)\n",
    "\n",
    "        logging.info(f\"Expected dataset size: {len(mapping)}\")\n",
    "        msgpack_path = config[f\"msgpack_{dataset_type}_dir\"]\n",
    "        image_ids_path = config[f\"{dataset_type}_meta_path\"]\n",
    "        dataset = MsgPackIterableMetaDataset(\n",
    "            msgpack_path,\n",
    "            image_ids_path,\n",
    "            image_ids_path,\n",
    "            key_img_id=config[\"key_img_id\"],\n",
    "            key_img_encoded=config[\"key_img_encoded\"],\n",
    "            ignore_image=True,\n",
    "        )\n",
    "\n",
    "        filtered_mapping = {}\n",
    "        for _, meta in dataset:\n",
    "            if meta[\"img_id\"] in mapping:\n",
    "                filtered_mapping[meta[\"img_id\"]] = mapping[meta[\"img_id\"]]\n",
    "        logging.info(f\"True dataset size: {len(filtered_mapping)}\")\n",
    "\n",
    "        with open(config[f\"{dataset_type}_label_mapping\"], \"w\") as fw:\n",
    "            json.dump(filtered_mapping, fw)\n",
    "    return\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"-c\", \"--config\", type=Path, default=\"config/baseM.yml\")\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "        datefmt=\"%d-%m-%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "\n",
    "    args = parse_args()\n",
    "\n",
    "    with open(args.config) as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    config = config[\"model_params\"]\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally this is from ```download_images.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "class MsgPackWriter:\n",
    "    def __init__(self, path, chunk_size=4096):\n",
    "        self.path = Path(path).absolute()\n",
    "        self.path.mkdir(parents=True, exist_ok=True)\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "        shards_re = r\"shard_(\\d+).msg\"\n",
    "        self.shards_index = [\n",
    "            int(re.match(shards_re, x).group(1))\n",
    "            for x in self.path.iterdir()\n",
    "            if x.is_dir() and re.match(shards_re, x)\n",
    "        ]\n",
    "        self.shard_open = None\n",
    "\n",
    "    def open_next(self):\n",
    "        if len(self.shards_index) == 0:\n",
    "            next_index = 0\n",
    "        else:\n",
    "            next_index = sorted(self.shards_index)[-1] + 1\n",
    "        self.shards_index.append(next_index)\n",
    "\n",
    "        if self.shard_open is not None and not self.shard_open.closed:\n",
    "            self.shard_open.close()\n",
    "\n",
    "        self.count = 0\n",
    "        self.shard_open = open(self.path / f\"shard_{next_index}.msg\", \"wb\")\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.open_next()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, tb):\n",
    "        self.shard_open.close()\n",
    "\n",
    "    def write(self, data):\n",
    "        if self.count >= self.chunk_size:\n",
    "            self.open_next()\n",
    "\n",
    "        self.shard_open.write(msgpack.packb(data))\n",
    "        self.count += 1\n",
    "\n",
    "\n",
    "def _thumbnail(img: PIL.Image, size: int) -> PIL.Image:\n",
    "    # resize an image maintaining the aspect ratio\n",
    "    # the smaller edge of the image will be matched to 'size'\n",
    "    w, h = img.size\n",
    "    if (w <= size) or (h <= size):\n",
    "        return img\n",
    "    if w < h:\n",
    "        ow = size\n",
    "        oh = int(size * h / w)\n",
    "        return img.resize((ow, oh), PIL.Image.BILINEAR)\n",
    "    else:\n",
    "        oh = size\n",
    "        ow = int(size * w / h)\n",
    "        return img.resize((ow, oh), PIL.Image.BILINEAR)\n",
    "\n",
    "\n",
    "def flickr_download(x, size_suffix=\"z\", min_edge_size=None):\n",
    "\n",
    "    # prevent downloading in full resolution using size_suffix\n",
    "    # https://www.flickr.com/services/api/misc.urls.html\n",
    "\n",
    "    image_id = x[\"image_id\"]\n",
    "    url_original = x[\"url\"]\n",
    "    if size_suffix != \"\":\n",
    "        url = url_original\n",
    "        # modify url to download image with specific size\n",
    "        ext = Path(url).suffix\n",
    "        url = f\"{url.split(ext)[0]}_{size_suffix}{ext}\"\n",
    "    else:\n",
    "        url = url_original\n",
    "\n",
    "    r = requests.get(url)\n",
    "    if r:\n",
    "        try:\n",
    "            image = PIL.Image.open(BytesIO(r.content))\n",
    "        except PIL.UnidentifiedImageError as e:\n",
    "            logger.error(f\"{image_id} : {url}: {e}\")\n",
    "            return\n",
    "    elif r.status_code == 129:\n",
    "        time.sleep(60)\n",
    "        logger.warning(\"To many requests, sleep for 60s...\")\n",
    "        flickr_download(x, min_edge_size=min_edge_size, size_suffix=size_suffix)\n",
    "    else:\n",
    "        logger.error(f\"{image_id} : {url}: {r.status_code}\")\n",
    "        return None\n",
    "\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    # resize if necessary\n",
    "    image = _thumbnail(image, min_edge_size)\n",
    "    # convert to jpeg\n",
    "    fp = BytesIO()\n",
    "    image.save(fp, \"JPEG\")\n",
    "\n",
    "    raw_bytes = fp.getvalue()\n",
    "    return {\"image\": raw_bytes, \"id\": image_id}\n",
    "\n",
    "\n",
    "class ImageDataloader:\n",
    "    def __init__(self, url_csv: Path, shuffle=False, nrows=None):\n",
    "\n",
    "        logger.info(\"Read dataset\")\n",
    "        self.df = pd.read_csv(\n",
    "            url_csv, names=[\"image_id\", \"url\"], header=None, nrows=nrows\n",
    "        )\n",
    "        # remove rows without url\n",
    "        self.df = self.df.dropna()\n",
    "        if shuffle:\n",
    "            logger.info(\"Shuffle images\")\n",
    "            self.df = self.df.sample(frac=1, random_state=10)\n",
    "        logger.info(f\"Number of URLs: {len(self.df.index)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for image_id, url in zip(self.df[\"image_id\"].values, self.df[\"url\"].values):\n",
    "            yield {\"image_id\": image_id, \"url\": url}\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    args = ArgumentParser()\n",
    "    args.add_argument(\n",
    "        \"--threads\",\n",
    "        type=int,\n",
    "        default=24,\n",
    "        help=\"Number of threads to download and process images\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"--output\",\n",
    "        type=Path,\n",
    "        default=Path(\"resources/images/mp16\"),\n",
    "        help=\"Output directory where images are stored\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"--url_csv\",\n",
    "        type=Path,\n",
    "        default=Path(\"resources/mp16_urls.csv\"),\n",
    "        help=\"CSV with Flickr image id and URL for downloading\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"--size\",\n",
    "        type=int,\n",
    "        default=320,\n",
    "        help=\"Rescale image to a minimum edge size of SIZE\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"--size_suffix\",\n",
    "        type=str,\n",
    "        default=\"z\",\n",
    "        help=\"Image size suffix according to the Flickr API; Empty string for original image\",\n",
    "    )\n",
    "    args.add_argument(\"--nrows\", type=int)\n",
    "    args.add_argument(\n",
    "        \"--shuffle\", action=\"store_true\", help=\"Shuffle list of URLs before downloading\"\n",
    "    )\n",
    "    return args.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    image_loader = ImageDataloader(args.url_csv, nrows=args.nrows, shuffle=args.shuffle)\n",
    "\n",
    "    counter_successful = 0\n",
    "    with Pool(args.threads) as p:\n",
    "        with MsgPackWriter(args.output) as f:\n",
    "            start = time.time()\n",
    "            for i, x in enumerate(\n",
    "                p.imap(\n",
    "                    partial(\n",
    "                        flickr_download,\n",
    "                        size_suffix=args.size_suffix,\n",
    "                        min_edge_size=args.size,\n",
    "                    ),\n",
    "                    image_loader,\n",
    "                )\n",
    "            ):\n",
    "                if x is None:\n",
    "                    continue\n",
    "\n",
    "                f.write(x)\n",
    "                counter_successful += 1\n",
    "\n",
    "                if i % 1000 == 0:\n",
    "                    end = time.time()\n",
    "                    logger.info(f\"{i}: {1000 / (end - start):.2f} image/s\")\n",
    "                    start = end\n",
    "    logger.info(\n",
    "        f\"Sucesfully downloaded {counter_successful}/{len(image_loader)} images ({counter_successful / len(image_loader):.3f})\"\n",
    "    )\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    args.output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logger = logging.getLogger(\"ImageDownloader\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    fh = logging.FileHandler(str(args.output / \"writer.log\"))\n",
    "    fh.setLevel(logging.INFO)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    sys.exit(main())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now following the github page we need:\n",
    "- item 1\n",
    "- item 2\n",
    "- item 3\n",
    "- item 4 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main link and paper that we need to follow is [this](https://github.com/TIBHannover/GeoEstimation) and [this](https://github.com/TIBHannover/GeoEstimation/releases/) for the pretrained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Davide ha trovato questo che forse è meglio [kaggle](https://www.kaggle.com/code/habedi/inspect-the-dataset/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qui ci sono dei links che potrebbero essere usati con colab col comando [!wget](https://qualinet.github.io/databases/image/world_wide_scale_geotagged_image_dataset_for_automatic_image_annotation_and_reverse_geotagging/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions inside Classification folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is ```inference.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    args = ArgumentParser()\n",
    "    args.add_argument(\n",
    "        \"--checkpoint\",\n",
    "        type=Path,\n",
    "        default=Path(\"models/base_M/epoch=014-val_loss=18.4833.ckpt\"),\n",
    "        help=\"Checkpoint to already trained model (*.ckpt)\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"--hparams\",\n",
    "        type=Path,\n",
    "        default=Path(\"models/base_M/hparams.yaml\"),\n",
    "        help=\"Path to hparams file (*.yaml) generated during training\",\n",
    "    )\n",
    "    args.add_argument(\n",
    "        \"--image_dir\",\n",
    "        type=Path,\n",
    "        default=Path(\"resources/images/im2gps\"),\n",
    "        help=\"Folder containing images. Supported file extensions: (*.jpg, *.jpeg, *.png)\",\n",
    "    )\n",
    "    # environment\n",
    "    args.add_argument(\n",
    "        \"--gpu\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Use GPU for inference if CUDA is available\",\n",
    "    )\n",
    "    args.add_argument(\"--batch_size\", type=int, default=64)\n",
    "    args.add_argument(\n",
    "        \"--num_workers\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Number of workers for image loading and pre-processing\",\n",
    "    )\n",
    "    return args.parse_args()\n",
    "\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "print(\"Load model from \", args.checkpoint)\n",
    "model = MultiPartitioningClassifier.load_from_checkpoint(\n",
    "    checkpoint_path=str(args.checkpoint),\n",
    "    hparams_file=str(args.hparams),\n",
    "    map_location=None,\n",
    ")\n",
    "model.eval()\n",
    "if args.gpu and torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "print(\"Init dataloader\")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    FiveCropImageDataset(meta_csv=None, image_dir=args.image_dir),\n",
    "    batch_size=ceil(args.batch_size / 5),\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    ")\n",
    "print(\"Number of images: \", len(dataloader.dataset))\n",
    "if len(dataloader.dataset) == 0:\n",
    "    raise RuntimeError(f\"No images found in {args.image_dir}\")\n",
    "\n",
    "rows = []\n",
    "for X in tqdm(dataloader):\n",
    "    if args.gpu:\n",
    "        X[0] = X[0].cuda()\n",
    "    img_paths, pred_classes, pred_latitudes, pred_longitudes = model.inference(X)\n",
    "    for p_key in pred_classes.keys():\n",
    "        for img_path, pred_class, pred_lat, pred_lng in zip(\n",
    "            img_paths,\n",
    "            pred_classes[p_key].cpu().numpy(),\n",
    "            pred_latitudes[p_key].cpu().numpy(),\n",
    "            pred_longitudes[p_key].cpu().numpy(),\n",
    "        ):\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"img_id\": Path(img_path).stem,\n",
    "                    \"p_key\": p_key,\n",
    "                    \"pred_class\": pred_class,\n",
    "                    \"pred_lat\": pred_lat,\n",
    "                    \"pred_lng\": pred_lng,\n",
    "                }\n",
    "            )\n",
    "df = pd.DataFrame.from_records(rows)\n",
    "df.set_index(keys=[\"img_id\", \"p_key\"], inplace=True)\n",
    "print(df)\n",
    "fout = Path(args.checkpoint).parent / f\"inference_{args.image_dir.stem}.csv\"\n",
    "print(\"Write output to\", fout)\n",
    "df.to_csv(fout)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is ```train_base.py``` which is huge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMultiPartitioningClassifier\u001b[39;00m(pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[0;32m      2\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, hparams: Namespace):\n\u001b[0;32m      3\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "class MultiPartitioningClassifier(pl.LightningModule):\n",
    "    def __init__(self, hparams: Namespace):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        self.partitionings, self.hierarchy = self.__init_partitionings()\n",
    "        self.model, self.classifier = self.__build_model()\n",
    "\n",
    "    def __init_partitionings(self):\n",
    "\n",
    "        partitionings = []\n",
    "        for shortname, path in zip(\n",
    "            self.hparams.partitionings[\"shortnames\"],\n",
    "            self.hparams.partitionings[\"files\"],\n",
    "        ):\n",
    "            partitionings.append(Partitioning(Path(path), shortname, skiprows=2))\n",
    "\n",
    "        if len(self.hparams.partitionings[\"files\"]) == 1:\n",
    "            return partitionings, None\n",
    "\n",
    "        return partitionings, Hierarchy(partitionings)\n",
    "\n",
    "    def __build_model(self):\n",
    "        logging.info(\"Build model\")\n",
    "        model, nfeatures = utils_global.build_base_model(self.hparams.arch)\n",
    "\n",
    "        classifier = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Linear(nfeatures, len(self.partitionings[i]))\n",
    "                for i in range(len(self.partitionings))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if self.hparams.weights:\n",
    "            logging.info(\"Load weights from pre-trained model\")\n",
    "            model, classifier = utils_global.load_weights_if_available(\n",
    "                model, classifier, self.hparams.weights\n",
    "            )\n",
    "\n",
    "        return model, classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        fv = self.model(x)\n",
    "        yhats = [self.classifier[i](fv) for i in range(len(self.partitionings))]\n",
    "        return yhats\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx=None):\n",
    "        images, target = batch\n",
    "\n",
    "        if not isinstance(target, list) and len(target.shape) == 1:\n",
    "            target = [target]\n",
    "\n",
    "        # forward pass\n",
    "        output = self(images)\n",
    "\n",
    "        # individual losses per partitioning\n",
    "        losses = [\n",
    "            torch.nn.functional.cross_entropy(output[i], target[i])\n",
    "            for i in range(len(output))\n",
    "        ]\n",
    "\n",
    "        loss = sum(losses)\n",
    "\n",
    "        # stats\n",
    "        losses_stats = {\n",
    "            f\"loss_train/{p}\": l\n",
    "            for (p, l) in zip([p.shortname for p in self.partitionings], losses)\n",
    "        }\n",
    "        for metric_name, metric_value in losses_stats.items():\n",
    "            self.log(metric_name, metric_value, prog_bar=True, logger=True)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, **losses_stats}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, target, true_lats, true_lngs = batch\n",
    "\n",
    "        if not isinstance(target, list) and len(target.shape) == 1:\n",
    "            target = [target]\n",
    "\n",
    "        # forward\n",
    "        output = self(images)\n",
    "\n",
    "        # loss calculation\n",
    "        losses = [\n",
    "            torch.nn.functional.cross_entropy(output[i], target[i])\n",
    "            for i in range(len(output))\n",
    "        ]\n",
    "\n",
    "        loss = sum(losses)\n",
    "\n",
    "        # log top-k accuracy for each partitioning\n",
    "        individual_accuracy_dict = utils_global.accuracy(\n",
    "            output, target, [p.shortname for p in self.partitionings]\n",
    "        )\n",
    "        # log loss for each partitioning\n",
    "        individual_loss_dict = {\n",
    "            f\"loss_val/{p}\": l\n",
    "            for (p, l) in zip([p.shortname for p in self.partitionings], losses)\n",
    "        }\n",
    "\n",
    "        # log GCD error@km threshold\n",
    "        distances_dict = {}\n",
    "\n",
    "        if self.hierarchy is not None:\n",
    "            hierarchy_logits = [\n",
    "                yhat[:, self.hierarchy.M[:, i]] for i, yhat in enumerate(output)\n",
    "            ]\n",
    "            hierarchy_logits = torch.stack(hierarchy_logits, dim=-1,)\n",
    "            hierarchy_preds = torch.prod(hierarchy_logits, dim=-1)\n",
    "\n",
    "        pnames = [p.shortname for p in self.partitionings]\n",
    "        if self.hierarchy is not None:\n",
    "            pnames.append(\"hierarchy\")\n",
    "        for i, pname in enumerate(pnames):\n",
    "            # get predicted coordinates\n",
    "            if i == len(self.partitionings):\n",
    "                i = i - 1\n",
    "                pred_class_indexes = torch.argmax(hierarchy_preds, dim=1)\n",
    "            else:\n",
    "                pred_class_indexes = torch.argmax(output[i], dim=1)\n",
    "            pred_latlngs = [\n",
    "                self.partitionings[i].get_lat_lng(idx)\n",
    "                for idx in pred_class_indexes.tolist()\n",
    "            ]\n",
    "            pred_lats, pred_lngs = map(list, zip(*pred_latlngs))\n",
    "            pred_lats = torch.tensor(pred_lats, dtype=torch.float)\n",
    "            pred_lngs = torch.tensor(pred_lngs, dtype=torch.float)\n",
    "            # calculate error\n",
    "            distances = utils_global.vectorized_gc_distance(\n",
    "                pred_lats,\n",
    "                pred_lngs,\n",
    "                true_lats.type_as(pred_lats),\n",
    "                true_lngs.type_as(pred_lats),\n",
    "            )\n",
    "            distances_dict[f\"gcd_{pname}_val\"] = distances\n",
    "\n",
    "        output = {\n",
    "            \"loss_val/total\": loss,\n",
    "            **individual_accuracy_dict,\n",
    "            **individual_loss_dict,\n",
    "            **distances_dict,\n",
    "        }\n",
    "        return output\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        pnames = [p.shortname for p in self.partitionings]\n",
    "\n",
    "        # top-k accuracy and loss per partitioning\n",
    "        loss_acc_dict = utils_global.summarize_loss_acc_stats(pnames, outputs)\n",
    "\n",
    "        # GCD stats per partitioning\n",
    "        gcd_dict = utils_global.summarize_gcd_stats(pnames, outputs, self.hierarchy)\n",
    "\n",
    "        metrics = {\n",
    "            \"val_loss\": loss_acc_dict[\"loss_val/total\"],\n",
    "            **loss_acc_dict,\n",
    "            **gcd_dict,\n",
    "        }\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            self.log(metric_name, metric_value, logger=True)\n",
    "\n",
    "    def _multi_crop_inference(self, batch):\n",
    "        images, meta_batch = batch\n",
    "        cur_batch_size = images.shape[0]\n",
    "        ncrops = images.shape[1]\n",
    "\n",
    "        # reshape crop dimension to batch\n",
    "        images = torch.reshape(images, (cur_batch_size * ncrops, *images.shape[2:]))\n",
    "\n",
    "        # forward pass\n",
    "        yhats = self(images)\n",
    "        yhats = [torch.nn.functional.softmax(yhat, dim=1) for yhat in yhats]\n",
    "\n",
    "        # respape back to access individual crops\n",
    "        yhats = [\n",
    "            torch.reshape(yhat, (cur_batch_size, ncrops, *list(yhat.shape[1:])))\n",
    "            for yhat in yhats\n",
    "        ]\n",
    "\n",
    "        # calculate max over crops\n",
    "        yhats = [torch.max(yhat, dim=1)[0] for yhat in yhats]\n",
    "\n",
    "        hierarchy_preds = None\n",
    "        if self.hierarchy is not None:\n",
    "            hierarchy_logits = torch.stack(\n",
    "                [yhat[:, self.hierarchy.M[:, i]] for i, yhat in enumerate(yhats)],\n",
    "                dim=-1,\n",
    "            )\n",
    "            hierarchy_preds = torch.prod(hierarchy_logits, dim=-1)\n",
    "\n",
    "        return yhats, meta_batch, hierarchy_preds\n",
    "\n",
    "    def inference(self, batch):\n",
    "\n",
    "        yhats, meta_batch, hierarchy_preds = self._multi_crop_inference(batch)\n",
    "\n",
    "        if self.hierarchy is not None:\n",
    "            nparts = len(self.partitionings) + 1\n",
    "        else:\n",
    "            nparts = len(self.partitionings)\n",
    "\n",
    "        pred_class_dict = {}\n",
    "        pred_lat_dict = {}\n",
    "        pred_lng_dict = {}\n",
    "        for i in range(nparts):\n",
    "            # get pred class indices\n",
    "            if self.hierarchy is not None and i == len(self.partitionings):\n",
    "                pname = \"hierarchy\"\n",
    "                pred_classes = torch.argmax(hierarchy_preds, dim=1)\n",
    "                i = i - 1\n",
    "            else:\n",
    "                pname = self.partitionings[i].shortname\n",
    "                pred_classes = torch.argmax(yhats[i], dim=1)\n",
    "\n",
    "            # calculate GCD\n",
    "            pred_lats, pred_lngs = map(\n",
    "                list,\n",
    "                zip(\n",
    "                    *[\n",
    "                        self.partitionings[i].get_lat_lng(c)\n",
    "                        for c in pred_classes.tolist()\n",
    "                    ]\n",
    "                ),\n",
    "            )\n",
    "            pred_lats = torch.tensor(pred_lats, dtype=torch.float)\n",
    "            pred_lngs = torch.tensor(pred_lngs, dtype=torch.float)\n",
    "            pred_lat_dict[pname] = pred_lats\n",
    "            pred_lng_dict[pname] = pred_lngs\n",
    "            pred_class_dict[pname] = pred_classes\n",
    "\n",
    "        return meta_batch[\"img_path\"], pred_class_dict, pred_lat_dict, pred_lng_dict\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "\n",
    "        yhats, meta_batch, hierarchy_preds = self._multi_crop_inference(batch)\n",
    "\n",
    "        distances_dict = {}\n",
    "        if self.hierarchy is not None:\n",
    "            nparts = len(self.partitionings) + 1\n",
    "        else:\n",
    "            nparts = len(self.partitionings)\n",
    "\n",
    "        for i in range(nparts):\n",
    "            # get pred class indices\n",
    "            if self.hierarchy is not None and i == len(self.partitionings):\n",
    "                pname = \"hierarchy\"\n",
    "                pred_classes = torch.argmax(hierarchy_preds, dim=1)\n",
    "                i = i - 1\n",
    "            else:\n",
    "                pname = self.partitionings[i].shortname\n",
    "                pred_classes = torch.argmax(yhats[i], dim=1)\n",
    "\n",
    "            # calculate GCD\n",
    "            pred_lats, pred_lngs = map(\n",
    "                list,\n",
    "                zip(\n",
    "                    *[\n",
    "                        self.partitionings[i].get_lat_lng(c)\n",
    "                        for c in pred_classes.tolist()\n",
    "                    ]\n",
    "                ),\n",
    "            )\n",
    "            pred_lats = torch.tensor(pred_lats, dtype=torch.float)\n",
    "            pred_lngs = torch.tensor(pred_lngs, dtype=torch.float)\n",
    "\n",
    "            distances = utils_global.vectorized_gc_distance(\n",
    "                pred_lats,\n",
    "                pred_lngs,\n",
    "                meta_batch[\"latitude\"].type_as(pred_lats),\n",
    "                meta_batch[\"longitude\"].type_as(pred_lngs),\n",
    "            )\n",
    "            distances_dict[pname] = distances\n",
    "\n",
    "        return distances_dict\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        result = utils_global.summarize_test_gcd(\n",
    "            [p.shortname for p in self.partitionings], outputs, self.hierarchy\n",
    "        )\n",
    "        return {**result}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optim_feature_extrator = torch.optim.SGD(\n",
    "            self.parameters(), **self.hparams.optim[\"params\"]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optim_feature_extrator,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": torch.optim.lr_scheduler.MultiStepLR(\n",
    "                    optim_feature_extrator, **self.hparams.scheduler[\"params\"]\n",
    "                ),\n",
    "                \"interval\": \"epoch\",\n",
    "                \"name\": \"lr\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def train_dataloader(self):\n",
    "\n",
    "        with open(self.hparams.train_label_mapping, \"r\") as f:\n",
    "            target_mapping = json.load(f)\n",
    "\n",
    "        tfm = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.RandomHorizontalFlip(),\n",
    "                torchvision.transforms.RandomResizedCrop(224, scale=(0.66, 1.0)),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(\n",
    "                    (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        dataset = MsgPackIterableDatasetMultiTargetWithDynLabels(\n",
    "            path=self.hparams.msgpack_train_dir,\n",
    "            target_mapping=target_mapping,\n",
    "            key_img_id=self.hparams.key_img_id,\n",
    "            key_img_encoded=self.hparams.key_img_encoded,\n",
    "            shuffle=True,\n",
    "            transformation=tfm,\n",
    "        )\n",
    "\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers_per_loader,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "\n",
    "        with open(self.hparams.val_label_mapping, \"r\") as f:\n",
    "            target_mapping = json.load(f)\n",
    "\n",
    "        tfm = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.Resize(256),\n",
    "                torchvision.transforms.CenterCrop(224),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(\n",
    "                    (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        dataset = MsgPackIterableDatasetMultiTargetWithDynLabels(\n",
    "            path=self.hparams.msgpack_val_dir,\n",
    "            target_mapping=target_mapping,\n",
    "            key_img_id=self.hparams.key_img_id,\n",
    "            key_img_encoded=self.hparams.key_img_encoded,\n",
    "            shuffle=False,\n",
    "            transformation=tfm,\n",
    "            meta_path=self.hparams.val_meta_path,\n",
    "            cache_size=1024,\n",
    "        )\n",
    "\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers_per_loader,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    args = ArgumentParser()\n",
    "    args.add_argument(\"-c\", \"--config\", type=Path, default=Path(\"config/baseM.yml\"))\n",
    "    args.add_argument(\"--progbar\", action=\"store_true\")\n",
    "    return args.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    with open(args.config) as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    model_params = config[\"model_params\"]\n",
    "    trainer_params = config[\"trainer_params\"]\n",
    "\n",
    "    utils_global.check_is_valid_torchvision_architecture(model_params[\"arch\"])\n",
    "\n",
    "    out_dir = Path(config[\"out_dir\"]) / datetime.now().strftime(\"%y%m%d-%H%M\")\n",
    "    out_dir.mkdir(exist_ok=True, parents=True)\n",
    "    logging.info(f\"Output directory: {out_dir}\")\n",
    "\n",
    "    # init classifier\n",
    "    model = MultiPartitioningClassifier(hparams=Namespace(**model_params))\n",
    "\n",
    "    logger = pl.loggers.TensorBoardLogger(save_dir=str(out_dir), name=\"tb_logs\")\n",
    "    checkpoint_dir = out_dir / \"ckpts\" / \"{epoch:03d}-{val_loss:.4f}\"\n",
    "    checkpointer = pl.callbacks.model_checkpoint.ModelCheckpoint(checkpoint_dir)\n",
    "\n",
    "    progress_bar_refresh_rate = 0\n",
    "    if args.progbar:\n",
    "        progress_bar_refresh_rate = 1\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        **trainer_params,\n",
    "        logger=logger,\n",
    "        val_check_interval=model_params[\"val_check_interval\"],\n",
    "        checkpoint_callback=checkpointer,\n",
    "        progress_bar_refresh_rate=progress_bar_refresh_rate,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "e583cc450fac4fd294c8076a1fbf5ede",
  "deepnote_persisted_session": {
   "createdAt": "2023-01-18T12:55:37.385Z"
  },
  "kernelspec": {
   "display_name": "vs-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "4fa9b9afc2baba5ccea1aafc34033c1dd8dcf2295c2dc2a369baeb32b0f17743"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
