{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87f7d3d6",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GianmarcoLattaruolo/Vision_Project/blob/main/test_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35233cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "os.chdir(cwd + r'\\GeoEstimation')\n",
    "sys.path.append(cwd + r'\\GeoEstimation')\n",
    "from pathlib import Path\n",
    "from math import ceil\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from classification.train_base import MultiPartitioningClassifier # class defining our model\n",
    "from classification.dataset import FiveCropImageDataset # class for preparing the images before giving them to the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3030239",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d442211f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'pytorch_lightning.core.lightning.LightningModule'>,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'abc.ABC'>, <class 'pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin'>, <class 'pytorch_lightning.core.grads.GradInformation'>, <class 'pytorch_lightning.core.saving.ModelIO'>, <class 'pytorch_lightning.core.hooks.ModelHooks'>, <class 'pytorch_lightning.core.hooks.DataHooks'>, <class 'pytorch_lightning.core.hooks.CheckpointHooks'>, <class 'torch.nn.modules.module.Module'>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'object'>,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_MultiPartitioningClassifier__build_model',\n",
       " '_MultiPartitioningClassifier__init_partitionings',\n",
       " '_multi_crop_inference',\n",
       " 'inference'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class attributes of pytorch_ligthning.LigthningModule:\t\t 50\n",
      "class attributes of MultiparitioningClassifier:\t\t\t 18\n",
      "instance attributes of pytorch_ligthning.LigthningModule:\t 26\n"
     ]
    }
   ],
   "source": [
    "#this cell is just to explore the number of attributes of the classes we have to work with\n",
    "methods_MultiPar = [method_name for method_name in dir(MultiPartitioningClassifier)\n",
    "                  if callable(getattr(MultiPartitioningClassifier, method_name))]\n",
    "display(len(methods_MultiPar))\n",
    "\n",
    "#MultiPartioningClassifier is child of pl.LightningModule\n",
    "print(MultiPartitioningClassifier.__bases__)\n",
    "methods_pl_Ligh = [method_name for method_name in dir(pl.LightningModule)\n",
    "                  if callable(getattr(pl.LightningModule, method_name))]\n",
    "display(len(methods_pl_Ligh))\n",
    "\n",
    "#pl.LightningModule is child of torch.nn.modules.module.Module and several other PyTorch lightning classes\n",
    "print(pl.LightningModule.__bases__)\n",
    "methods_pytorch_nn = [method_name for method_name in dir(torch.nn.modules.module.Module)\n",
    "                  if callable(getattr(torch.nn.modules.module.Module, method_name))]\n",
    "display(len(methods_pytorch_nn))\n",
    "\n",
    "#torch.nn.modules.module.Module is not a child class\n",
    "print(torch.nn.modules.module.Module.__bases__)\n",
    "\n",
    "#only 4 attributes/methods from MultiPartitioningClassifier are new w.r.t. pl.LightningModule\n",
    "#but I guess some are overwritten\n",
    "display(set(methods_MultiPar)-set(methods_pl_Ligh)) \n",
    "display(set(methods_pl_Ligh)-set(methods_MultiPar))\n",
    "\n",
    "print(\"class attributes of pytorch_ligthning.LigthningModule:\"+2*\"\\t\",len(pl.LightningModule.__dict__.keys())) #class methods\n",
    "print(\"class attributes of MultiparitioningClassifier:\"+3*\"\\t\",len(MultiPartitioningClassifier.__dict__.keys()))\n",
    "M1 = pl.LightningModule()\n",
    "print(\"instance attributes of pytorch_ligthning.LigthningModule:\"+\"\\t\",len(M1.__dict__.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34a91b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "INFO:lightning:GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "# where model's params and hyperparams are saved\n",
    "checkpoint = \"models/base_M/epoch=014-val_loss=18.4833.ckpt\"\n",
    "hparams = \"models/base_M/hparams.yaml\"\n",
    "# load_from_checkpoint is a static method from pytorch lightning, inherited by MultiPartitioningClassifier\n",
    "# it permits to load a model previously saved, in the form of a checkpoint file, and one with hyperparameters\n",
    "# MultiPartitioningClassifier is the class defining our model\n",
    "model = MultiPartitioningClassifier.load_from_checkpoint(\n",
    "    checkpoint_path=checkpoint,\n",
    "    hparams_file=hparams,\n",
    "    map_location=None,\n",
    "    stric = False #Whether to strictly enforce that the keys in checkpoint_path match\n",
    "    # the keys returned by this module’s state dict.\n",
    ")\n",
    "#I put some the function's variables from the documentation, with some comments\n",
    "wanted_precision = 32\n",
    "trainer = pl.Trainer(callbacks=None, #Add a callback or list of callbacks.\n",
    "                     gradient_clip_val=None, #The value at which to clip gradients. Passing gradient_clip_val=None disables gradient clipping\n",
    "                     track_grad_norm= -1, #-1 = no track, otherwise tracks the p-norm. May be set to ‘inf’ infinity-norm. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them. \n",
    "                     check_val_every_n_epoch=1, # Perform a validation loop every after every N training epochs.\n",
    "                     max_epochs=None, # Stop training once this number of epochs is reached. Disabled by default (None). If both max_epochs and max_steps are not specified, defaults to max_epochs = 1000. To enable infinite training, set max_epochs = -1.\n",
    "                     max_steps = -1, #Stop training after this number of steps. \n",
    "                     log_every_n_steps=50, #How often to log within steps. Default: 50\n",
    "                     accelerator=None, # different accelerator types (“cpu”, “gpu”, “tpu”, “ipu”, “hpu”, “mps, “auto”)\n",
    "                     precision=wanted_precision, #Double precision (64), full precision (32), half precision (16) or bfloat16 precision (bf16).\n",
    "                     resume_from_checkpoint=None, #Deprecated since version v1.5:use Trainer.fit(..., ckpt_path=...) instead.\n",
    "                     auto_lr_find=False, #If set to True, will make trainer.tune() run a learning rate finder, trying to optimize initial learning for faster convergence.\n",
    "                     auto_scale_batch_size=False) #If set to True, will initially run a batch size finder trying to find the largest batch size that fits into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "846f5745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photo_id</th>\n",
       "      <th>owner</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>faves</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>u_city</th>\n",
       "      <th>u_country</th>\n",
       "      <th>taken</th>\n",
       "      <th>weather</th>\n",
       "      <th>season</th>\n",
       "      <th>daytime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17271526139</td>\n",
       "      <td>130418712@N05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rio Trejo</td>\n",
       "      <td>Son numerosos los rios y arroyos que discurren...</td>\n",
       "      <td>701.0</td>\n",
       "      <td>36,861544</td>\n",
       "      <td>-5,177747</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-04-26 17:11:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17776887679</td>\n",
       "      <td>55101137@N02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-05-13-022FD PH-XRD</td>\n",
       "      <td>&lt;u&gt;&lt;b&gt;Aircraft Type - Registration - (c/n)&lt;/b&gt;...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51,463766</td>\n",
       "      <td>5,392935</td>\n",
       "      <td>Bodmin</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2015-05-13 00:00:22</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17898331633</td>\n",
       "      <td>55101137@N02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-05-17-022FD OO-GWA</td>\n",
       "      <td>&lt;u&gt;&lt;b&gt;Aircraft Type - Registration - (c/n)&lt;/b&gt;...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>51,190492</td>\n",
       "      <td>4,453765</td>\n",
       "      <td>Bodmin</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2015-05-17 00:00:22</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17940239919</td>\n",
       "      <td>55101137@N02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-05-14-020FD D-1553</td>\n",
       "      <td>&lt;u&gt;&lt;b&gt;Aircraft Type - Registration - (c/n)&lt;/b&gt;...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51,326247</td>\n",
       "      <td>6,085953</td>\n",
       "      <td>Bodmin</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2015-05-14 00:00:20</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17963122505</td>\n",
       "      <td>55101137@N02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-05-13-025FD EI-DLI</td>\n",
       "      <td>&lt;u&gt;&lt;b&gt;Aircraft Type - Registration - (c/n)&lt;/b&gt;...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>51,463766</td>\n",
       "      <td>5,392935</td>\n",
       "      <td>Bodmin</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2015-05-13 00:00:25</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      photo_id          owner  gender occupation                    title  \\\n",
       "0  17271526139  130418712@N05     1.0        NaN                Rio Trejo   \n",
       "1  17776887679   55101137@N02     1.0        NaN  2015-05-13-022FD PH-XRD   \n",
       "2  17898331633   55101137@N02     1.0        NaN  2015-05-17-022FD OO-GWA   \n",
       "3  17940239919   55101137@N02     1.0        NaN  2015-05-14-020FD D-1553   \n",
       "4  17963122505   55101137@N02     1.0        NaN  2015-05-13-025FD EI-DLI   \n",
       "\n",
       "                                         description  faves        lat  \\\n",
       "0  Son numerosos los rios y arroyos que discurren...  701.0  36,861544   \n",
       "1  <u><b>Aircraft Type - Registration - (c/n)</b>...    1.0  51,463766   \n",
       "2  <u><b>Aircraft Type - Registration - (c/n)</b>...    2.0  51,190492   \n",
       "3  <u><b>Aircraft Type - Registration - (c/n)</b>...    0.0  51,326247   \n",
       "4  <u><b>Aircraft Type - Registration - (c/n)</b>...    2.0  51,463766   \n",
       "\n",
       "         lon  u_city       u_country                taken  weather  season  \\\n",
       "0  -5,177747     NaN             NaN  2015-04-26 17:11:11      NaN     1.0   \n",
       "1   5,392935  Bodmin  United Kingdom  2015-05-13 00:00:22      9.0     1.0   \n",
       "2   4,453765  Bodmin  United Kingdom  2015-05-17 00:00:22      9.0     1.0   \n",
       "3   6,085953  Bodmin  United Kingdom  2015-05-14 00:00:20      9.0     1.0   \n",
       "4   5,392935  Bodmin  United Kingdom  2015-05-13 00:00:25      9.0     1.0   \n",
       "\n",
       "   daytime  \n",
       "0      2.0  \n",
       "1      3.0  \n",
       "2      3.0  \n",
       "3      3.0  \n",
       "4      3.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.flickr.com/photos/130418712@N05/17271526139\n",
      "https://www.flickr.com/photos/55101137@N02/17776887679\n",
      "https://www.flickr.com/photos/55101137@N02/17898331633\n",
      "https://www.flickr.com/photos/55101137@N02/17940239919\n",
      "https://www.flickr.com/photos/55101137@N02/17963122505\n",
      "https://www.flickr.com/photos/55101137@N02/18331250198\n",
      "https://www.flickr.com/photos/69258414@N08/20052582226\n",
      "https://www.flickr.com/photos/131084835@N03/20383414776\n",
      "https://www.flickr.com/photos/133755874@N03/25740797653\n",
      "https://www.flickr.com/photos/32675973@N00/25843872864\n",
      "https://www.flickr.com/photos/130418712@N05/26503578166\n",
      "https://www.flickr.com/photos/130418712@N05/27624094755\n",
      "https://www.flickr.com/photos/69258414@N08/28853122201\n",
      "https://www.flickr.com/photos/146106459@N03/29263898094\n",
      "https://www.flickr.com/photos/130418712@N05/29499158291\n",
      "https://www.flickr.com/photos/146106459@N03/29742232315\n",
      "https://www.flickr.com/photos/146106459@N03/29777481322\n",
      "https://www.flickr.com/photos/146106459@N03/29807819351\n",
      "https://www.flickr.com/photos/133755874@N03/29884547414\n",
      "https://www.flickr.com/photos/8704943@N07/31536395197\n",
      "https://www.flickr.com/photos/8704943@N07/31536395467\n",
      "https://www.flickr.com/photos/8704943@N07/31536395917\n",
      "https://www.flickr.com/photos/25722132@N05/31958928478\n",
      "https://www.flickr.com/photos/67878469@N00/32050747657\n",
      "https://www.flickr.com/photos/79986881@N00/32390620857\n",
      "https://www.flickr.com/photos/79986881@N00/32390624347\n",
      "https://www.flickr.com/photos/83776910@N06/32409873957\n",
      "https://www.flickr.com/photos/94185526@N04/32415092967\n",
      "https://www.flickr.com/photos/81035653@N00/32421137367\n",
      "https://www.flickr.com/photos/35110249@N05/32436483927\n",
      "https://www.flickr.com/photos/20747318@N08/32438407777\n",
      "https://www.flickr.com/photos/20747318@N08/32438408037\n",
      "https://www.flickr.com/photos/158438800@N02/32438412857\n",
      "https://www.flickr.com/photos/158438800@N02/32438413367\n",
      "https://www.flickr.com/photos/158438800@N02/32438413807\n",
      "https://www.flickr.com/photos/158438800@N02/32438414497\n",
      "https://www.flickr.com/photos/158438800@N02/32438414657\n",
      "https://www.flickr.com/photos/20747318@N08/32438455347\n",
      "https://www.flickr.com/photos/20747318@N08/32438455457\n",
      "https://www.flickr.com/photos/20747318@N08/32438455627\n"
     ]
    }
   ],
   "source": [
    "csv10k = r'C:\\Users\\latta\\GitHub\\Vision_Project\\GeoEstimation\\resources\\images\\landscape10k\\data_10k.csv'\n",
    "new_data_urls = pd.read_csv(csv10k, sep = ';')\n",
    "display(new_data_urls.head())\n",
    "#ok è molto più difficile trovare le foto da questo file. L'url della foto ti spiaga come trovarlo qui\n",
    "#https://www.flickr.com/services/api/misc.urls.html\n",
    "#sometimes https://www.flickr.com/photos/{user-id}/{photo-id} works sometimes no\n",
    "ss = 'https://www.flickr.com/photos/'\n",
    "new_data_urls['url']=[ss + '/'.join((str(i[0]),str(i[1]))) for i in zip(new_data_urls['owner'], new_data_urls['photo_id'])]\n",
    "for i in new_data_urls['url'].head(40):\n",
    "    print(i)\n",
    "\n",
    "# a questo punto imparando ad usare il loro file download_images.py dovremmo farcela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc50a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIO BANANA\n",
      "Read resources\\images\\im2gps_places365.csv\n",
      "                                        img_id        author   latitude  \\\n",
      "0     104123223_7410c654ba_19_19355699@N00.jpg  19355699@N00 -16.663606   \n",
      "1   1095548455_f636d22cbb_1277_8576809@N08.jpg   8576809@N08  31.893581   \n",
      "2  1185597181_0158ab4213_1311_43616936@N00.jpg  43616936@N00  42.346571   \n",
      "3  1199004207_0ce4e7a456_1285_16418049@N00.jpg  16418049@N00  37.090924   \n",
      "4  1257001714_3453f5fc4b_1405_11490799@N08.jpg  11490799@N08  55.485759   \n",
      "\n",
      "    longitude  s3_label  s16_label  s365_label  prob_indoor  prob_natural  \\\n",
      "0  145.563537         1          8         150     0.002959      0.777815   \n",
      "1  -85.141124         2         15         231     0.003976      0.016128   \n",
      "2  -71.097228         2         12         312     0.000005      0.000004   \n",
      "3   25.370521         2         15         227     0.056002      0.007563   \n",
      "4   28.791046         1          6         205     0.000083      0.991441   \n",
      "\n",
      "   prob_urban  \n",
      "0    0.219226  \n",
      "1    0.979896  \n",
      "2    0.999991  \n",
      "3    0.936435  \n",
      "4    0.008475  \n",
      "                                        img_id        author   latitude  \\\n",
      "0     104123223_7410c654ba_19_19355699@N00.jpg  19355699@N00 -16.663606   \n",
      "1   1095548455_f636d22cbb_1277_8576809@N08.jpg   8576809@N08  31.893581   \n",
      "2  1185597181_0158ab4213_1311_43616936@N00.jpg  43616936@N00  42.346571   \n",
      "3  1199004207_0ce4e7a456_1285_16418049@N00.jpg  16418049@N00  37.090924   \n",
      "4  1257001714_3453f5fc4b_1405_11490799@N08.jpg  11490799@N08  55.485759   \n",
      "\n",
      "    longitude  s3_label  s16_label  s365_label  prob_indoor  prob_natural  \\\n",
      "0  145.563537         1          8         150     0.002959      0.777815   \n",
      "1  -85.141124         2         15         231     0.003976      0.016128   \n",
      "2  -71.097228         2         12         312     0.000005      0.000004   \n",
      "3   25.370521         2         15         227     0.056002      0.007563   \n",
      "4   28.791046         1          6         205     0.000083      0.991441   \n",
      "\n",
      "   prob_urban                                           img_path  \n",
      "0    0.219226  resources\\images\\im2gps\\104123223_7410c654ba_1...  \n",
      "1    0.979896  resources\\images\\im2gps\\1095548455_f636d22cbb_...  \n",
      "2    0.999991  resources\\images\\im2gps\\1185597181_0158ab4213_...  \n",
      "3    0.936435  resources\\images\\im2gps\\1199004207_0ce4e7a456_...  \n",
      "4    0.008475  resources\\images\\im2gps\\1257001714_3453f5fc4b_...  \n",
      "(<class 'torch.utils.data.dataset.Dataset'>,)\n"
     ]
    }
   ],
   "source": [
    "# I want to train on the second 3k-images test set\n",
    "image_dir = r\"resources\\images\\im2gps\"\n",
    "meta_csv = r\"resources\\images\\im2gps_places365.csv\"\n",
    "#FiveCropImageDataset is the class for preparing the images before giving them to the NN\n",
    "# in particular, it creates five different crops for every image\n",
    "dataset = FiveCropImageDataset(meta_csv, image_dir)\n",
    "# NOTA: in realtà il Five-Cropping avviene solo nel momento in cui si chiama dataset[idx]\n",
    "# the authors created this classe from torch.utils.data.dataset.Dataset class\n",
    "print(FiveCropImageDataset.__bases__)\n",
    "batch_size = 64\n",
    "#Data loader. Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "                    dataset = dataset, #dataset from which to load the data.\n",
    "                    batch_size=ceil(batch_size / 5),  #you divide by 5 because for each image you generate 5 different crops\n",
    "                    shuffle=False, # set to True to have the data reshuffled at every epoch (default: False).\n",
    "                    num_workers=4, #number ot threads used for parallelism (cores of CPU?) \n",
    "                    #how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)\n",
    "                    pin_memory=False, #If True, the data loader will copy tensors into CUDA pinned memory before returning them.\n",
    "                    drop_last=False, #set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size.\n",
    "                    timeout=0 # if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: 0)               \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97ad2f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'classification.dataset.FiveCropImageDataset'>\n",
      "237\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'dict'>\n",
      "{'img_id': '104123223_7410c654ba_19_19355699@N00.jpg', 'author': '19355699@N00', 'latitude': -16.663606, 'longitude': 145.56353700000003, 's3_label': 1, 's16_label': 8, 's365_label': 150, 'prob_indoor': 0.002959289950443811, 'prob_natural': 0.7778147804293098, 'prob_urban': 0.219225829974576, 'img_path': 'resources\\\\images\\\\im2gps\\\\104123223_7410c654ba_19_19355699@N00.jpg'}\n",
      "torch.Size([5, 3, 224, 224])\n",
      "                                        img_id        author   latitude  \\\n",
      "0     104123223_7410c654ba_19_19355699@N00.jpg  19355699@N00 -16.663606   \n",
      "1   1095548455_f636d22cbb_1277_8576809@N08.jpg   8576809@N08  31.893581   \n",
      "2  1185597181_0158ab4213_1311_43616936@N00.jpg  43616936@N00  42.346571   \n",
      "3  1199004207_0ce4e7a456_1285_16418049@N00.jpg  16418049@N00  37.090924   \n",
      "4  1257001714_3453f5fc4b_1405_11490799@N08.jpg  11490799@N08  55.485759   \n",
      "\n",
      "    longitude  s3_label  s16_label  s365_label  prob_indoor  prob_natural  \\\n",
      "0  145.563537         1          8         150     0.002959      0.777815   \n",
      "1  -85.141124         2         15         231     0.003976      0.016128   \n",
      "2  -71.097228         2         12         312     0.000005      0.000004   \n",
      "3   25.370521         2         15         227     0.056002      0.007563   \n",
      "4   28.791046         1          6         205     0.000083      0.991441   \n",
      "\n",
      "   prob_urban                                           img_path  \n",
      "0    0.219226  resources\\images\\im2gps\\104123223_7410c654ba_1...  \n",
      "1    0.979896  resources\\images\\im2gps\\1095548455_f636d22cbb_...  \n",
      "2    0.999991  resources\\images\\im2gps\\1185597181_0158ab4213_...  \n",
      "3    0.936435  resources\\images\\im2gps\\1199004207_0ce4e7a456_...  \n",
      "4    0.008475  resources\\images\\im2gps\\1257001714_3453f5fc4b_...  \n",
      "<class 'tuple'>\n",
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's explore the dataset \n",
    "print(type(dataset))\n",
    "print(len(dataset)) #forse sono 2997 e non 300 perchè hanno detto che non prendono più foto dello stesso autore...\n",
    "print(type(dataset[0]))\n",
    "print(len(dataset[0]))\n",
    "print(type(dataset[0][0]))\n",
    "print(type(dataset[0][1]))\n",
    "print(dataset[0][1])\n",
    "print(dataset[0][0].shape)\n",
    "print(dataset.meta_info.head())\n",
    "print(type(dataset.__getitem__(0)))\n",
    "print(sum(sum(sum(sum(dataset[0][0]!=dataset.__getitem__(0)[0]))))) # __getimtem__ ti tira fuori la tupla di due elementi:\n",
    "# il torch tensor dell'immagine e il dizionario dei vari dati (tipo gps) dell'immagine.\n",
    "dataset.tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d6ed7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | model      | Sequential | 23 M  \n",
      "1 | classifier | ModuleList | 47 M  \n",
      "INFO:lightning:\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | model      | Sequential | 23 M  \n",
      "1 | classifier | ModuleList | 47 M  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 17\u001b[0m\n\u001b[0;32m      5\u001b[0m val_dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(\n\u001b[0;32m      6\u001b[0m                     dataset \u001b[39m=\u001b[39m val_data, \u001b[39m#dataset from which to load the data.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m                     batch_size\u001b[39m=\u001b[39mceil(batch_size \u001b[39m/\u001b[39m \u001b[39m5\u001b[39m),  \u001b[39m#you divide by 5 because for each image you generate 5 different crops\u001b[39;00m\n\u001b[0;32m      8\u001b[0m                     num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, \u001b[39m#number ot threads used for parallelism (cores of CPU?) \u001b[39;00m\n\u001b[0;32m      9\u001b[0m                     \u001b[39m#how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m                 )\n\u001b[0;32m     11\u001b[0m train_dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(\n\u001b[0;32m     12\u001b[0m                     dataset \u001b[39m=\u001b[39m train_data, \u001b[39m#dataset from which to load the data.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m                     batch_size\u001b[39m=\u001b[39mceil(batch_size \u001b[39m/\u001b[39m \u001b[39m5\u001b[39m),  \u001b[39m#you divide by 5 because for each image you generate 5 different crops\u001b[39;00m\n\u001b[0;32m     14\u001b[0m                     num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, \u001b[39m#number ot threads used for parallelism (cores of CPU?) \u001b[39;00m\n\u001b[0;32m     15\u001b[0m                     \u001b[39m#how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m                 )\n\u001b[1;32m---> 17\u001b[0m new_training \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mfit(model \u001b[39m=\u001b[39;49m model, \u001b[39m#model to  fit\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m                            train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader, \u001b[39m# Pytorch DataLoader with training samples. \u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m                            \u001b[39m#If the model has a predefined train_dataloader method this will be skipped \u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m                            val_dataloaders\u001b[39m=\u001b[39;49mval_dataloader, \u001b[39m#Either a single Pytorch Dataloader or a list of them, \u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m                            \u001b[39m# specifying validation samples. If the model has a predefined val_dataloaders \u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m                            \u001b[39m# method this will be skipped\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m                            datamodule\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\vs-project\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:440\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[39m# TRAIN\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m'\u001b[39m\u001b[39mon_fit_start\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 440\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator_backend\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator_backend\u001b[39m.\u001b[39mteardown()\n\u001b[0;32m    443\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[39m# hook\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\vs-project\\lib\\site-packages\\pytorch_lightning\\accelerators\\cpu_accelerator.py:48\u001b[0m, in \u001b[0;36mCPUAccelerator.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtrain_loop\u001b[39m.\u001b[39msetup_training(model)\n\u001b[0;32m     47\u001b[0m \u001b[39m# train or test\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_or_test()\n\u001b[0;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\vs-project\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py:66\u001b[0m, in \u001b[0;36mAccelerator.train_or_test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mrun_test()\n\u001b[0;32m     65\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     67\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\vs-project\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:462\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 462\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_sanity_check(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_model())\n\u001b[0;32m    464\u001b[0m     \u001b[39m# enable train mode\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_model()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\vs-project\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:648\u001b[0m, in \u001b[0;36mTrainer.run_sanity_check\u001b[1;34m(self, ref_model)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_sanity_check_start()\n\u001b[0;32m    647\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m--> 648\u001b[0m _, eval_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_evaluation(test_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, max_batches\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_sanity_val_batches)\n\u001b[0;32m    650\u001b[0m \u001b[39m# allow no returns from eval\u001b[39;00m\n\u001b[0;32m    651\u001b[0m \u001b[39mif\u001b[39;00m eval_results \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(eval_results) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    652\u001b[0m     \u001b[39m# when we get a list back, used only the last item\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\vs-project\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:568\u001b[0m, in \u001b[0;36mTrainer.run_evaluation\u001b[1;34m(self, test_mode, max_batches)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\u001b[39m.\u001b[39mon_evaluation_batch_start(batch, batch_idx, dataloader_idx)\n\u001b[0;32m    567\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[1;32m--> 568\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluation_loop\u001b[39m.\u001b[39;49mevaluation_step(test_mode, batch, batch_idx, dataloader_idx)\n\u001b[0;32m    569\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\u001b[39m.\u001b[39mevaluation_step_end(output)\n\u001b[0;32m    571\u001b[0m \u001b[39m# hook\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\vs-project\\lib\\site-packages\\pytorch_lightning\\trainer\\evaluation_loop.py:171\u001b[0m, in \u001b[0;36mEvaluationLoop.evaluation_step\u001b[1;34m(self, test_mode, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[0;32m    169\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39maccelerator_backend\u001b[39m.\u001b[39mtest_step(args)\n\u001b[0;32m    170\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 171\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator_backend\u001b[39m.\u001b[39;49mvalidation_step(args)\n\u001b[0;32m    173\u001b[0m \u001b[39m# track batch size for weighted average\u001b[39;00m\n\u001b[0;32m    174\u001b[0m is_result_obj \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(output, Result)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\vs-project\\lib\\site-packages\\pytorch_lightning\\accelerators\\cpu_accelerator.py:64\u001b[0m, in \u001b[0;36mCPUAccelerator.validation_step\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     62\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mvalidation_step(\u001b[39m*\u001b[39margs)\n\u001b[0;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\GitHub\\Vision_Project\\GeoEstimation\\classification\\train_base.py:92\u001b[0m, in \u001b[0;36mMultiPartitioningClassifier.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m---> 92\u001b[0m     images, target, true_lats, true_lngs \u001b[39m=\u001b[39m batch\n\u001b[0;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(target, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(target\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     95\u001b[0m         target \u001b[39m=\u001b[39m [target]\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "#we need to specify the validation data since we don't have the file:\n",
    "#'resources/yfcc_25600_places365_mapping_h3.json'\n",
    "c = 0.1 #ratio for validation set\n",
    "val_data, train_data = torch.utils.data.random_split(dataset, [int(c*len(dataset)),len(dataset)-int(c*len(dataset))])\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "                    dataset = val_data, #dataset from which to load the data.\n",
    "                    batch_size=ceil(batch_size / 5),  #you divide by 5 because for each image you generate 5 different crops\n",
    "                    num_workers=4, #number ot threads used for parallelism (cores of CPU?) \n",
    "                    #how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)\n",
    "                )\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "                    dataset = train_data, #dataset from which to load the data.\n",
    "                    batch_size=ceil(batch_size / 5),  #you divide by 5 because for each image you generate 5 different crops\n",
    "                    num_workers=4, #number ot threads used for parallelism (cores of CPU?) \n",
    "                    #how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)\n",
    "                )\n",
    "new_training = trainer.fit(model = model, #model to  fit\n",
    "                           train_dataloader=train_dataloader, # Pytorch DataLoader with training samples. \n",
    "                           #If the model has a predefined train_dataloader method this will be skipped \n",
    "                           val_dataloaders=val_dataloader, #Either a single Pytorch Dataloader or a list of them, \n",
    "                           # specifying validation samples. If the model has a predefined val_dataloaders \n",
    "                           # method this will be skipped\n",
    "                           datamodule=None)#A instance of LightningDataModule, optional\n",
    "#this gives the error: ValueError: not enough values to unpack (expected 4, got 2) we both the 2 datasets we have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab34b0a",
   "metadata": {},
   "source": [
    "# Some useful links\n",
    "\n",
    "[load_from_checkpoints](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.core.saving.ModelIO.html)\n",
    "\n",
    "[pytorch.Trainer](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html)\n",
    "\n",
    "[transfer learning](https://pytorch-lightning.readthedocs.io/en/stable/advanced/finetuning.html)\n",
    "\n",
    "[pytorch lightning 1.0.1 full documentation](https://pytorch-lightning.readthedocs.io/_/downloads/en/1.0.1/pdf/)\n",
    "Unfortunately we need to watch this since several functions arguments have changed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "4fa9b9afc2baba5ccea1aafc34033c1dd8dcf2295c2dc2a369baeb32b0f17743"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
