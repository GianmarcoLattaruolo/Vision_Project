{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87f7d3d6",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GianmarcoLattaruolo/Vision_Project/blob/main/test_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35233cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "os.chdir(cwd + r'\\GeoEstimation')\n",
    "sys.path.append(cwd + r'\\GeoEstimation')\n",
    "from pathlib import Path\n",
    "from math import ceil\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from classification.train_base import MultiPartitioningClassifier # class defining our model\n",
    "from classification.dataset import FiveCropImageDataset # class for preparing the images before giving them to the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3030239",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d442211f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'pytorch_lightning.core.lightning.LightningModule'>,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'abc.ABC'>, <class 'pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin'>, <class 'pytorch_lightning.core.grads.GradInformation'>, <class 'pytorch_lightning.core.saving.ModelIO'>, <class 'pytorch_lightning.core.hooks.ModelHooks'>, <class 'pytorch_lightning.core.hooks.DataHooks'>, <class 'pytorch_lightning.core.hooks.CheckpointHooks'>, <class 'torch.nn.modules.module.Module'>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'object'>,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_MultiPartitioningClassifier__build_model',\n",
       " '_MultiPartitioningClassifier__init_partitionings',\n",
       " '_multi_crop_inference',\n",
       " 'inference'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class attributes of pytorch_ligthning.LigthningModule:\t\t 50\n",
      "class attributes of MultiparitioningClassifier:\t\t\t 18\n",
      "instance attributes of pytorch_ligthning.LigthningModule:\t 26\n"
     ]
    }
   ],
   "source": [
    "#this cell is just to explore the number of attributes of the classes we have to work with\n",
    "methods_MultiPar = [method_name for method_name in dir(MultiPartitioningClassifier)\n",
    "                  if callable(getattr(MultiPartitioningClassifier, method_name))]\n",
    "display(len(methods_MultiPar))\n",
    "\n",
    "#MultiPartioningClassifier is child of pl.LightningModule\n",
    "print(MultiPartitioningClassifier.__bases__)\n",
    "methods_pl_Ligh = [method_name for method_name in dir(pl.LightningModule)\n",
    "                  if callable(getattr(pl.LightningModule, method_name))]\n",
    "display(len(methods_pl_Ligh))\n",
    "\n",
    "#pl.LightningModule is child of torch.nn.modules.module.Module and several other PyTorch lightning classes\n",
    "print(pl.LightningModule.__bases__)\n",
    "methods_pytorch_nn = [method_name for method_name in dir(torch.nn.modules.module.Module)\n",
    "                  if callable(getattr(torch.nn.modules.module.Module, method_name))]\n",
    "display(len(methods_pytorch_nn))\n",
    "\n",
    "#torch.nn.modules.module.Module is not a child class\n",
    "print(torch.nn.modules.module.Module.__bases__)\n",
    "\n",
    "#only 4 attributes/methods from MultiPartitioningClassifier are new w.r.t. pl.LightningModule\n",
    "#but I guess some are overwritten\n",
    "display(set(methods_MultiPar)-set(methods_pl_Ligh)) \n",
    "display(set(methods_pl_Ligh)-set(methods_MultiPar))\n",
    "\n",
    "print(\"class attributes of pytorch_ligthning.LigthningModule:\"+2*\"\\t\",len(pl.LightningModule.__dict__.keys())) #class methods\n",
    "print(\"class attributes of MultiparitioningClassifier:\"+3*\"\\t\",len(MultiPartitioningClassifier.__dict__.keys()))\n",
    "M1 = pl.LightningModule()\n",
    "print(\"instance attributes of pytorch_ligthning.LigthningModule:\"+\"\\t\",len(M1.__dict__.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34a91b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "INFO:lightning:GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "# where model's params and hyperparams are saved\n",
    "checkpoint = \"models/base_M/epoch=014-val_loss=18.4833.ckpt\"\n",
    "hparams = \"models/base_M/hparams.yaml\"\n",
    "# load_from_checkpoint is a static method from pytorch lightning, inherited by MultiPartitioningClassifier\n",
    "# it permits to load a model previously saved, in the form of a checkpoint file, and one with hyperparameters\n",
    "# MultiPartitioningClassifier is the class defining our model\n",
    "model = MultiPartitioningClassifier.load_from_checkpoint(\n",
    "    checkpoint_path=checkpoint,\n",
    "    hparams_file=hparams,\n",
    "    map_location=None,\n",
    "    stric = False #Whether to strictly enforce that the keys in checkpoint_path match\n",
    "    # the keys returned by this module’s state dict.\n",
    ")\n",
    "#I put some the function's variables from the documentation, with some comments\n",
    "wanted_precision = 32\n",
    "trainer = pl.Trainer(callbacks=None, #Add a callback or list of callbacks.\n",
    "                     gradient_clip_val=None, #The value at which to clip gradients. Passing gradient_clip_val=None disables gradient clipping\n",
    "                     track_grad_norm= -1, #-1 = no track, otherwise tracks the p-norm. May be set to ‘inf’ infinity-norm. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them. \n",
    "                     check_val_every_n_epoch=1, # Perform a validation loop every after every N training epochs.\n",
    "                     max_epochs=None, # Stop training once this number of epochs is reached. Disabled by default (None). If both max_epochs and max_steps are not specified, defaults to max_epochs = 1000. To enable infinite training, set max_epochs = -1.\n",
    "                     max_steps = -1, #Stop training after this number of steps. \n",
    "                     log_every_n_steps=50, #How often to log within steps. Default: 50\n",
    "                     accelerator=None, # different accelerator types (“cpu”, “gpu”, “tpu”, “ipu”, “hpu”, “mps, “auto”)\n",
    "                     precision=wanted_precision, #Double precision (64), full precision (32), half precision (16) or bfloat16 precision (bf16).\n",
    "                     resume_from_checkpoint=None, #Deprecated since version v1.5:use Trainer.fit(..., ckpt_path=...) instead.\n",
    "                     auto_lr_find=False, #If set to True, will make trainer.tune() run a learning rate finder, trying to optimize initial learning for faster convergence.\n",
    "                     auto_scale_batch_size=False) #If set to True, will initially run a batch size finder trying to find the largest batch size that fits into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4cf635a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photo_id</th>\n",
       "      <th>owner</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>faves</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>u_city</th>\n",
       "      <th>u_country</th>\n",
       "      <th>taken</th>\n",
       "      <th>weather</th>\n",
       "      <th>season</th>\n",
       "      <th>daytime</th>\n",
       "      <th>base_url</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17271526139</td>\n",
       "      <td>130418712@N05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rio Trejo</td>\n",
       "      <td>Son numerosos los rios y arroyos que discurren...</td>\n",
       "      <td>701.0</td>\n",
       "      <td>36,861544</td>\n",
       "      <td>-5,177747</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-04-26 17:11:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://www.flickr.com/photos/130418712@N05/17...</td>\n",
       "      <td>https://live.staticflickr.com/65535/1727152613...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17776887679</td>\n",
       "      <td>55101137@N02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-05-13-022FD PH-XRD</td>\n",
       "      <td>&lt;u&gt;&lt;b&gt;Aircraft Type - Registration - (c/n)&lt;/b&gt;...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51,463766</td>\n",
       "      <td>5,392935</td>\n",
       "      <td>Bodmin</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2015-05-13 00:00:22</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>https://www.flickr.com/photos/55101137@N02/177...</td>\n",
       "      <td>https://live.staticflickr.com/5335/17776887679...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      photo_id          owner  gender occupation                    title  \\\n",
       "0  17271526139  130418712@N05     1.0        NaN                Rio Trejo   \n",
       "1  17776887679   55101137@N02     1.0        NaN  2015-05-13-022FD PH-XRD   \n",
       "\n",
       "                                         description  faves        lat  \\\n",
       "0  Son numerosos los rios y arroyos que discurren...  701.0  36,861544   \n",
       "1  <u><b>Aircraft Type - Registration - (c/n)</b>...    1.0  51,463766   \n",
       "\n",
       "         lon  u_city       u_country                taken  weather  season  \\\n",
       "0  -5,177747     NaN             NaN  2015-04-26 17:11:11      NaN     1.0   \n",
       "1   5,392935  Bodmin  United Kingdom  2015-05-13 00:00:22      9.0     1.0   \n",
       "\n",
       "   daytime                                           base_url  \\\n",
       "0      2.0  https://www.flickr.com/photos/130418712@N05/17...   \n",
       "1      3.0  https://www.flickr.com/photos/55101137@N02/177...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://live.staticflickr.com/65535/1727152613...  \n",
       "1  https://live.staticflickr.com/5335/17776887679...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.read_csv(r'resources\\images\\new_data10k\\final_dataset.csv', sep = ';', index_col = 0)\n",
    "new_data.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da47f3c2",
   "metadata": {},
   "source": [
    "Consider that the last letter before \".jpg\" encodes the image dimension. Cfr:\n",
    "- t\tthumbnail\t100\t\n",
    "- m\tsmall\t240\t\n",
    "- n\tsmall\t320\t\n",
    "- w\tsmall\t400\t\n",
    "- (none)\tmedium\t500\t\n",
    "- z\tmedium\t640\t\n",
    "- c\tmedium\t800\t\n",
    "- b\tlarge\t1024\n",
    "- o\toriginal\tarbitrary\thas a unique secret; photo owner can restrict; files have full EXIF data; files might not be rotated; files can use an arbitrary file extension \n",
    "\n",
    "In the paper they used as default for the donwload the 'z' size.\n",
    "     \n",
    "```python\n",
    "url = url[:-5]+size_suffix+url[-4:]\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a6c9e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del loro file download_images.py possiamo utilizzare la function flickr_download\n",
    "# (che a sua volta richiede _thumbnail ) cambiando alcune cosette\n",
    "import PIL\n",
    "import time\n",
    "from PIL import ImageFile\n",
    "from io import BytesIO\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "#this is ok I think\n",
    "def _thumbnail(img: PIL.Image, size: int) -> PIL.Image:\n",
    "    # resize an image maintaining the aspect ratio\n",
    "    # the smaller edge of the image will be matched to 'size'\n",
    "    w, h = img.size\n",
    "    if (w <= size) or (h <= size):\n",
    "        return img\n",
    "    if w < h:\n",
    "        ow = size\n",
    "        oh = int(size * h / w)\n",
    "        return img.resize((ow, oh), PIL.Image.BILINEAR)\n",
    "    else:\n",
    "        oh = size\n",
    "        ow = int(size * w / h)\n",
    "        return img.resize((ow, oh), PIL.Image.BILINEAR)\n",
    "\n",
    "#this should be modified\n",
    "def flickr_download(url,image_id, size_suffix=\"z\", min_edge_size=400):\n",
    "\n",
    "    if url!=np.nan:\n",
    "        url_original = url\n",
    "        if size_suffix != \"\":\n",
    "            url = url[:-5]+size_suffix+url[-4:] \n",
    "        else:\n",
    "            url = url_original\n",
    "\n",
    "        r = requests.get(url)\n",
    "        if r:\n",
    "            try:\n",
    "                image = PIL.Image.open(BytesIO(r.content))\n",
    "                image.show()\n",
    "            except PIL.UnidentifiedImageError as e:\n",
    "                print('error')\n",
    "                #logger.error(f\"{image_id} : {url}: {e}\")\n",
    "                return\n",
    "        elif r.status_code == 129:\n",
    "            print('time_sleep')\n",
    "            time.sleep(60)\n",
    "            #sto sostituendo i logger con i print\n",
    "            print(\"To many requests, sleep for 60s...\") # mette in pausa la funzione poi chiama se stessa\n",
    "            flickr_download(x, min_edge_size=min_edge_size, size_suffix=size_suffix)\n",
    "        else:\n",
    "            print(f\"{image_id} : {url}: {r.status_code}\")\n",
    "            return None\n",
    "\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "\n",
    "        # resize if necessary\n",
    "        image = _thumbnail(image, min_edge_size)  #we should know which are the input dimensions\n",
    "        # convert to jpeg\n",
    "        fp = BytesIO()\n",
    "        image.save(r'C:\\Users\\latta\\GitHub\\Vision_Project\\GeoEstimation\\resources\\images\\new_data10k','PNG')# fp can be a filename (string), pathlib.Path object or file object.\n",
    "\n",
    "        raw_bytes = fp.getvalue()# questo non so a che serve\n",
    "        return #{\"image\": raw_bytes, \"id\": image_id} #questo return non so a che serve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d1cadd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\latta\\GitHub\\Vision_Project\\GeoEstimation\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "#our defult working directory is Geoestimation\n",
    "\n",
    "def download_image(url, file_path, file_name, size = 'z'):\n",
    "    url = url[:-5]+size+url[-4:]\n",
    "    file_name = str(file_name)\n",
    "    full_path = file_path + '/'+ file_name + '.jpg'\n",
    "    try: \n",
    "        urllib.request.urlretrieve(url, full_path)\n",
    "        return 'ok'\n",
    "    except:\n",
    "        print(f'the url {url} does not work')\n",
    "        return ''\n",
    "\n",
    "\n",
    "def download_from_dataframe(df, num_photos=250):\n",
    "    os.chdir(r'resources\\images\\new_data10k')\n",
    "    cwd = os.getcwd()\n",
    "    count = 0\n",
    "    start = os.listdir()\n",
    "    if len(start)>10006:\n",
    "        print('Dataset already downloaded')\n",
    "        return\n",
    "    for i,url in enumerate(df['url']):\n",
    "        id = str(df['photo_id'][i])\n",
    "        \n",
    "        if id+'.jpg' not in start and count<num_photos and type(url)!=float:\n",
    "            status = download_image(url, cwd , id)\n",
    "            if status=='ok':\n",
    "                count += 1\n",
    "    #this is to return to the original parent folder\n",
    "    os.chdir(r'..')\n",
    "    os.chdir(r'..')\n",
    "    os.chdir(r'..')\n",
    "    return \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4f2afd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\latta\\GitHub\\Vision_Project\\GeoEstimation\n"
     ]
    }
   ],
   "source": [
    "download_from_dataframe(new_data, num_photos = 5)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc50a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIO BANANA\n",
      "Read resources\\images\\im2gps_places365.csv\n",
      "                                        img_id        author   latitude  \\\n",
      "0     104123223_7410c654ba_19_19355699@N00.jpg  19355699@N00 -16.663606   \n",
      "1   1095548455_f636d22cbb_1277_8576809@N08.jpg   8576809@N08  31.893581   \n",
      "2  1185597181_0158ab4213_1311_43616936@N00.jpg  43616936@N00  42.346571   \n",
      "3  1199004207_0ce4e7a456_1285_16418049@N00.jpg  16418049@N00  37.090924   \n",
      "4  1257001714_3453f5fc4b_1405_11490799@N08.jpg  11490799@N08  55.485759   \n",
      "\n",
      "    longitude  s3_label  s16_label  s365_label  prob_indoor  prob_natural  \\\n",
      "0  145.563537         1          8         150     0.002959      0.777815   \n",
      "1  -85.141124         2         15         231     0.003976      0.016128   \n",
      "2  -71.097228         2         12         312     0.000005      0.000004   \n",
      "3   25.370521         2         15         227     0.056002      0.007563   \n",
      "4   28.791046         1          6         205     0.000083      0.991441   \n",
      "\n",
      "   prob_urban  \n",
      "0    0.219226  \n",
      "1    0.979896  \n",
      "2    0.999991  \n",
      "3    0.936435  \n",
      "4    0.008475  \n",
      "                                        img_id        author   latitude  \\\n",
      "0     104123223_7410c654ba_19_19355699@N00.jpg  19355699@N00 -16.663606   \n",
      "1   1095548455_f636d22cbb_1277_8576809@N08.jpg   8576809@N08  31.893581   \n",
      "2  1185597181_0158ab4213_1311_43616936@N00.jpg  43616936@N00  42.346571   \n",
      "3  1199004207_0ce4e7a456_1285_16418049@N00.jpg  16418049@N00  37.090924   \n",
      "4  1257001714_3453f5fc4b_1405_11490799@N08.jpg  11490799@N08  55.485759   \n",
      "\n",
      "    longitude  s3_label  s16_label  s365_label  prob_indoor  prob_natural  \\\n",
      "0  145.563537         1          8         150     0.002959      0.777815   \n",
      "1  -85.141124         2         15         231     0.003976      0.016128   \n",
      "2  -71.097228         2         12         312     0.000005      0.000004   \n",
      "3   25.370521         2         15         227     0.056002      0.007563   \n",
      "4   28.791046         1          6         205     0.000083      0.991441   \n",
      "\n",
      "   prob_urban                                           img_path  \n",
      "0    0.219226  resources\\images\\im2gps\\104123223_7410c654ba_1...  \n",
      "1    0.979896  resources\\images\\im2gps\\1095548455_f636d22cbb_...  \n",
      "2    0.999991  resources\\images\\im2gps\\1185597181_0158ab4213_...  \n",
      "3    0.936435  resources\\images\\im2gps\\1199004207_0ce4e7a456_...  \n",
      "4    0.008475  resources\\images\\im2gps\\1257001714_3453f5fc4b_...  \n",
      "(<class 'torch.utils.data.dataset.Dataset'>,)\n"
     ]
    }
   ],
   "source": [
    "# I want to train on the second 3k-images test set\n",
    "image_dir = r\"resources\\images\\im2gps\"\n",
    "meta_csv = r\"resources\\images\\im2gps_places365.csv\"\n",
    "#FiveCropImageDataset is the class for preparing the images before giving them to the NN\n",
    "# in particular, it creates five different crops for every image\n",
    "dataset = FiveCropImageDataset(meta_csv, image_dir)\n",
    "# NOTA: in realtà il Five-Cropping avviene solo nel momento in cui si chiama dataset[idx]\n",
    "# the authors created this classe from torch.utils.data.dataset.Dataset class\n",
    "print(FiveCropImageDataset.__bases__)\n",
    "batch_size = 64\n",
    "#Data loader. Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "                    dataset = dataset, #dataset from which to load the data.\n",
    "                    batch_size=ceil(batch_size / 5),  #you divide by 5 because for each image you generate 5 different crops\n",
    "                    shuffle=False, # set to True to have the data reshuffled at every epoch (default: False).\n",
    "                    num_workers=4, #number ot threads used for parallelism (cores of CPU?) \n",
    "                    #how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)\n",
    "                    pin_memory=False, #If True, the data loader will copy tensors into CUDA pinned memory before returning them.\n",
    "                    drop_last=False, #set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size.\n",
    "                    timeout=0 # if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: 0)               \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97ad2f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'classification.dataset.FiveCropImageDataset'>\n",
      "237\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'dict'>\n",
      "{'img_id': '104123223_7410c654ba_19_19355699@N00.jpg', 'author': '19355699@N00', 'latitude': -16.663606, 'longitude': 145.56353700000003, 's3_label': 1, 's16_label': 8, 's365_label': 150, 'prob_indoor': 0.002959289950443811, 'prob_natural': 0.7778147804293098, 'prob_urban': 0.219225829974576, 'img_path': 'resources\\\\images\\\\im2gps\\\\104123223_7410c654ba_19_19355699@N00.jpg'}\n",
      "torch.Size([5, 3, 224, 224])\n",
      "                                        img_id        author   latitude  \\\n",
      "0     104123223_7410c654ba_19_19355699@N00.jpg  19355699@N00 -16.663606   \n",
      "1   1095548455_f636d22cbb_1277_8576809@N08.jpg   8576809@N08  31.893581   \n",
      "2  1185597181_0158ab4213_1311_43616936@N00.jpg  43616936@N00  42.346571   \n",
      "3  1199004207_0ce4e7a456_1285_16418049@N00.jpg  16418049@N00  37.090924   \n",
      "4  1257001714_3453f5fc4b_1405_11490799@N08.jpg  11490799@N08  55.485759   \n",
      "\n",
      "    longitude  s3_label  s16_label  s365_label  prob_indoor  prob_natural  \\\n",
      "0  145.563537         1          8         150     0.002959      0.777815   \n",
      "1  -85.141124         2         15         231     0.003976      0.016128   \n",
      "2  -71.097228         2         12         312     0.000005      0.000004   \n",
      "3   25.370521         2         15         227     0.056002      0.007563   \n",
      "4   28.791046         1          6         205     0.000083      0.991441   \n",
      "\n",
      "   prob_urban                                           img_path  \n",
      "0    0.219226  resources\\images\\im2gps\\104123223_7410c654ba_1...  \n",
      "1    0.979896  resources\\images\\im2gps\\1095548455_f636d22cbb_...  \n",
      "2    0.999991  resources\\images\\im2gps\\1185597181_0158ab4213_...  \n",
      "3    0.936435  resources\\images\\im2gps\\1199004207_0ce4e7a456_...  \n",
      "4    0.008475  resources\\images\\im2gps\\1257001714_3453f5fc4b_...  \n",
      "<class 'tuple'>\n",
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's explore the dataset \n",
    "print(type(dataset))\n",
    "print(len(dataset)) #forse sono 2997 e non 300 perchè hanno detto che non prendono più foto dello stesso autore...\n",
    "print(type(dataset[0]))\n",
    "print(len(dataset[0]))\n",
    "print(type(dataset[0][0]))\n",
    "print(type(dataset[0][1]))\n",
    "print(dataset[0][1])\n",
    "print(dataset[0][0].shape)\n",
    "print(dataset.meta_info.head())\n",
    "print(type(dataset.__getitem__(0)))\n",
    "print(sum(sum(sum(sum(dataset[0][0]!=dataset.__getitem__(0)[0]))))) # __getimtem__ ti tira fuori la tupla di due elementi:\n",
    "# il torch tensor dell'immagine e il dizionario dei vari dati (tipo gps) dell'immagine.\n",
    "dataset.tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02011b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check out the use of this command\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "t, v = random_split(range(10), [3,7] , generator=torch.GeneratorExit().manual_seed(42))\n",
    "print(list(t))\n",
    "print(list(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ed7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to specify the validation data since we don't have the file:\n",
    "#'resources/yfcc_25600_places365_mapping_h3.json'\n",
    "c = 0.1 #ratio for validation set\n",
    "val_data, train_data = random_split(dataset, [int(c*len(dataset)),len(dataset)-int(c*len(dataset))])\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "                    dataset = val_data, #dataset from which to load the data.\n",
    "                    batch_size=ceil(batch_size / 5),  #you divide by 5 because for each image you generate 5 different crops\n",
    "                    num_workers=4, #number ot threads used for parallelism (cores of CPU?) \n",
    "                    #how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)\n",
    "                )\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "                    dataset = train_data, #dataset from which to load the data.\n",
    "                    batch_size=ceil(batch_size / 5),  #you divide by 5 because for each image you generate 5 different crops\n",
    "                    num_workers=4, #number ot threads used for parallelism (cores of CPU?) \n",
    "                    #how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)\n",
    "                )\n",
    "new_training = trainer.fit(model = model, #model to  fit\n",
    "                           train_dataloader=train_dataloader, # Pytorch DataLoader with training samples. \n",
    "                           #If the model has a predefined train_dataloader method this will be skipped \n",
    "                           val_dataloaders=val_dataloader, #Either a single Pytorch Dataloader or a list of them, \n",
    "                           # specifying validation samples. If the model has a predefined val_dataloaders \n",
    "                           # method this will be skipped\n",
    "                           datamodule=None)#A instance of LightningDataModule, optional\n",
    "#this gives the error: ValueError: not enough values to unpack (expected 4, got 2) we both the 2 datasets we have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab34b0a",
   "metadata": {},
   "source": [
    "# Some useful links\n",
    "\n",
    "[load_from_checkpoints](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.core.saving.ModelIO.html)\n",
    "\n",
    "[pytorch.Trainer](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html)\n",
    "\n",
    "[transfer learning](https://pytorch-lightning.readthedocs.io/en/stable/advanced/finetuning.html)\n",
    "\n",
    "[pytorch lightning 1.0.1 full documentation](https://pytorch-lightning.readthedocs.io/_/downloads/en/1.0.1/pdf/)\n",
    "Unfortunately we need to watch this since several functions arguments have changed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "4fa9b9afc2baba5ccea1aafc34033c1dd8dcf2295c2dc2a369baeb32b0f17743"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
