{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35233cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from math import ceil\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from classification.train_base import MultiPartitioningClassifier # class defining our model\n",
    "from classification.dataset import FiveCropImageDataset # class for preparing the images before giving them to the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3030239",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a08e673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where model's params and hyperparams are saved\n",
    "checkpoint = \"models/base_M/epoch=014-val_loss=18.4833.ckpt\"\n",
    "hparams = \"models/base_M/hparams.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a91b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_from_checkpoint is a static method from pytorch lightning, inherited by MultiPartitioningClassifier\n",
    "# it permits to load a model previously saved, in the form of a checkpoint file, and one with hyperparameters\n",
    "# MultiPartitioningClassifier is the class defining our model\n",
    "model = MultiPartitioningClassifier.load_from_checkpoint(\n",
    "    checkpoint_path=checkpoint,\n",
    "    hparams_file=hparams,\n",
    "    map_location=None\n",
    ")\n",
    "\n",
    "#model.eval() to see the model's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "421e7e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "INFO:lightning:GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "#to allow GPU\n",
    "want_gpu = True\n",
    "if want_gpu and torch.cuda.is_available():\n",
    "    gpu = 1\n",
    "else:\n",
    "    gpu = None\n",
    "\n",
    "# the class Trainer from pythorch lightining is the one responsible for training a deep NN\n",
    "# it can initialize the model, run forward and backward passes, optimize, print stats, early stop...\n",
    "wanted_precision = 32 #16 for half precision (how many bits for each number)\n",
    "trainer = pl.Trainer(gpus=gpu, precision=wanted_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f1b57",
   "metadata": {},
   "source": [
    "## Load and initialize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bc50a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where images are saved\n",
    "image_dir = \"resources/images/im2gps\"\n",
    "meta_csv = \"resources/images/im2gps_places365.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d581ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read resources/images/im2gps_places365.csv\n"
     ]
    }
   ],
   "source": [
    "#FiveCropImageDataset is the class for preparing the images before giving them to the NN\n",
    "# in particular, it creates five different crops for every image\n",
    "dataset = FiveCropImageDataset(meta_csv, image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78fb535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "                    dataset,\n",
    "                    batch_size=ceil(batch_size / 5),  #you divide by 5 because for each image you generate 5 different crops\n",
    "                    shuffle=False,\n",
    "                    num_workers=4 #number ot threads used for parallelism (cores of CPU?)\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a027ce75",
   "metadata": {},
   "source": [
    "## Run the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d6ed7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [02:49<00:00,  8.93s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db/anaconda3/envs/geoestimation-github-pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The testing_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "results = trainer.test(model, test_dataloaders=dataloader, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc77d2",
   "metadata": {},
   "source": [
    "## Look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae2292a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  1         25        200       750       2500\n",
      "partitioning                                                  \n",
      "coarse        0.092827  0.316456  0.497890  0.670886  0.789030\n",
      "middle        0.139241  0.345992  0.481013  0.683544  0.793249\n",
      "fine          0.156118  0.392405  0.489451  0.658228  0.784810\n",
      "hierarchy     0.147679  0.375527  0.489451  0.683544  0.789030\n"
     ]
    }
   ],
   "source": [
    "# formatting results into a pandas dataframe\n",
    "df = pd.DataFrame(results[0]).T\n",
    "#df[\"dataset\"] = image_dir\n",
    "df[\"partitioning\"] = df.index\n",
    "df[\"partitioning\"] = df[\"partitioning\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "df.set_index(keys=[\"partitioning\"], inplace=True) #keys=[\"dataset\", \"partitioning\"] in case\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3813e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the dataframe on a csv file\n",
    "fout = 'test_results.csv'\n",
    "df.to_csv(fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
